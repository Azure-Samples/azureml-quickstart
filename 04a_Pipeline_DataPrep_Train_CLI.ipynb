{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Create production machine learning pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of a machine learning pipeline is to split a complete machine learning task into a multistep workflow. Each step is a manageable component that can be developed, optimized, configured, and automated individually. Steps are connected through well-defined interfaces. The Azure Machine Learning pipeline service automatically orchestrates all the dependencies between pipeline steps. The benefits of using a pipeline are standardized the MLOps practice, scalable team collaboration, training efficiency and cost reduction. To learn more about the benefits of pipelines, see [What are Azure Machine Learning pipelines](https://learn.microsoft.comazure/machine-learning/concept-ml-pipelines).\n",
    "\n",
    "In this tutorial, you use Azure Machine Learning to create a production ready machine learning project, using Azure Machine Learning Python SDK v2.\n",
    "\n",
    "This means you will be able to leverage the AzureML Python SDK to:\n",
    "\n",
    "- Get a handle to your Azure Machine Learning workspace\n",
    "- Create Azure Machine Learning data assets\n",
    "- Create reusable Azure Machine Learning components\n",
    "- Create, validate and run Azure Machine Learning pipelines\n",
    "\n",
    "During this tutorial, you create an Azure Machine Learning pipeline to train a model for credit default prediction. The pipeline handles two steps: \n",
    "\n",
    "1. Data preparation\n",
    "1. Training and registering the trained model\n",
    "\n",
    "The next image shows a simple pipeline as you'll see it in the Azure studio once submitted.\n",
    "\n",
    "![Screenshot that shows the AML Pipeline](./media/pipeline-overview.jpg \"Overview of the pipeline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load local environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, environment variables used during the exercise are stored in a .env file.  Please create or modify your local .env file prior to the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) If not already installed, install environment management package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load local environment from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo WORKSPACE_NAME=%WORKSPACE_NAME%\n",
    "!echo RESOURCE_GROUP_NAME=%RESOURCE_GROUP_NAME%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure Azure ML session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now necessary to configure local Azure ML session to point to the appropriate workspace and resource group.  The following code configures local CLI environment for subsequent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az configure --defaults workspace=%WORKSPACE_NAME% group=%RESOURCE_GROUP_NAME%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the pipeline resources\n",
    "\n",
    "The Azure Machine Learning framework can be used from CLI, Python SDK, or studio interface. In this example, you use the Azure Machine Learning Python SDK v2 to create a pipeline. \n",
    "\n",
    "Before creating the pipeline, you need the following resources:\n",
    "\n",
    "* The data asset for training\n",
    "* The software environment to run the pipeline\n",
    "* A compute resource to where the job runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the registered data asset using CLI\n",
    "\n",
    "Start by getting the data that you previously registered in [Tutorial: Upload, access and explore your data](explore-data.ipynb).\n",
    "\n",
    "* Azure Machine Learning uses a `Data` object to register a reusable definition of data, and consume data within a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the first time that you're making a call to the workspace, you may be asked to authenticate. Once the authentication is complete, you then see the dataset registration completion message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml data show --name \"credit-card_csv\" --version \"2023.10.05.154542\" --resource-group %RESOURCE_GROUP_NAME% --workspace-name %WORKSPACE_NAME% | findstr path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a compute resource to run your pipeline (Optional)\n",
    "\n",
    "You can **skip this step** if you want to use [serverless compute (preview)](https://learn.microsoft.com/azure/machine-learning/how-to-use-serverless-compute?view=azureml-api-2&tabs=python) to run the training job. Through serverless compute, Azure Machine Learning takes care of creating, scaling, deleting, patching and managing compute, along with providing managed network isolation, reducing the burden on you. \n",
    "\n",
    "Each step of an Azure Machine Learning pipeline can use a different compute resource for running the specific job of that step. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
    "\n",
    "In this section, you provision a Linux  [compute cluster](https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python). See the [full list on VM sizes and prices](https://azure.microsoft.com/en-ca/pricing/details/machine-learning/) .\n",
    "\n",
    "For this tutorial, you only need a basic cluster so use a Standard_DS3_v2 model with 2 vCPU cores, 7-GB RAM and create an Azure Machine Learning Compute.\n",
    "> [!TIP]\n",
    "> If you already have a compute cluster, replace \"cpu-cluster\" in the next code block with the name of your cluster.  This will keep you from creating another one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLUSTER_NAME cpu-cluster\n",
    "!az ml compute create -n %CLUSTER_NAME% --type amlcompute \\\n",
    "    --min-instances 0 \\\n",
    "    --max-instances 4 \\\n",
    "    --size STANDARD_DS3_V2 \\\n",
    "    --idle-time-before-scale-down 180 \\\n",
    "    --tier Dedicated \\\n",
    "    --resource-group %RESOURCE_GROUP_NAME% \\\n",
    "    --workspace-name %WORKSPACE_NAME%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "==========================================================================================\n",
    "# OPTIONAL- Run this if you have not done previous steps!\n",
    "## Create a job environment for pipeline steps. \n",
    "\n",
    "So far, you've created a development environment on the compute instance, your development machine. You also need an environment to use for each step of the pipeline. Each step can have its own environment, or you can use some common environments for multiple steps.\n",
    "\n",
    "In this example, you create a conda environment for your jobs, using a conda yaml file.\n",
    "First, create a directory to store the file in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "dependencies_dir"
   },
   "outputs": [],
   "source": [
    "!mkdir dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the file in the dependencies directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "conda.yaml"
   },
   "outputs": [],
   "source": [
    "%%writefile components\\conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 2.4.1\n",
    "    - azureml-mlflow==1.51.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specification contains some usual packages, that you use in your pipeline (numpy, pip), together with some Azure Machine Learning specific packages (azureml-mlflow).\n",
    "\n",
    "The Azure Machine Learning packages aren't mandatory to run Azure Machine Learning jobs. However, adding these packages let you interact with Azure Machine Learning for logging metrics and registering models, all inside the Azure Machine Learning job. You use them in the training script later in this tutorial.\n",
    "\n",
    "Use the *yaml* file to create and register this custom environment in your workspace:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Optional Steps\n",
    "==========================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training pipeline\n",
    "\n",
    "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself.\n",
    "\n",
    "Azure Machine Learning pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
    "\n",
    "- Write the yaml specification of the component, or create it programmatically using the `az component` CLI command.\n",
    "- Load that component from the pipeline code.\n",
    "- Implement the pipeline using the component's inputs, outputs and parameters.\n",
    "- Submit the pipeline.\n",
    "\n",
    "There are two ways to create a component, programmatic and yaml definition. The next two sections walk you through creating a component both ways. You can either create the two components trying both options or pick your preferred method.\n",
    "\n",
    "> [!NOTE]\n",
    "> In this tutorial for simplicity we are using the same compute for all components. However, you can set different computes for each component, for example by adding a line like `\"compute: azureml:cpu-cluster\"`. To view an example of building a pipeline with different computes for each component, see the [Basic pipeline job section in the cifar-10 pipeline tutorial](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/2b_train_cifar_10_with_pytorch/train_cifar_10_with_pytorch.ipynb).\n",
    "\n",
    "### Create component 1: data prep (using SDK programmatic definition)\n",
    "\n",
    "Let's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prep.py* Python file.\n",
    "\n",
    "First create a source folder for the data_prep component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "data_prep_src_dir"
   },
   "outputs": [],
   "source": [
    "%env DATA_PREP_SRC_DIR components\\data_prep\n",
    "!mkdir %DATA_PREP_SRC_DIR%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script performs the simple task of splitting the data into train and test datasets. Azure Machine Learning mounts datasets as folders to the computes, therefore, we created an auxiliary `select_first_file` function to access the data file inside the mounted input folder. \n",
    "\n",
    "[MLFlow](https://learn.microsoft.com/articles/machine-learning/concept-mlflow) is used to log the parameters and metrics during our pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "def-main"
   },
   "outputs": [],
   "source": [
    "%%writefile components\\data_prep\\data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--train_data_csv\", type=str, help=\"name of train data\")\n",
    "    parser.add_argument(\"--test_data_csv\", type=str, help=\"name of test data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "\n",
    "    credit_df = pd.read_csv(args.data, header=1, index_col=0)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
    "\n",
    "    credit_train_df, credit_test_df = train_test_split(\n",
    "        credit_df,\n",
    "        test_size=args.test_train_ratio,\n",
    "    )\n",
    "\n",
    "    os.makedirs(args.train_data_csv, exist_ok=True)\n",
    "    os.makedirs(args.test_data_csv, exist_ok=True)\n",
    "\n",
    "    print(\"???!!!\", args.train_data_csv)\n",
    "\n",
    "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
    "    # train_data_path = os.path.join(args.train_data, \"data.csv\")\n",
    "    # print(\"train_data_path\", os.path.abspath(train_data_path))\n",
    "\n",
    "\n",
    "    credit_train_df.to_csv(os.path.join(os.getcwd(), args.train_data_csv, \"data.csv\"), index=False)\n",
    "\n",
    "    # test_data_path = os.path.join(args.test_data, \"data.csv\")\n",
    "    # print(\"test_data_path\", os.path.abspath(test_data_path))\n",
    "    credit_test_df.to_csv(os.path.join(os.getcwd(), args.test_data_csv, \"data.csv\"), index=False)\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a script that can perform the desired task, create an Azure Machine Learning Component from it.\n",
    "\n",
    "Use the general purpose `component` that can run command line actions. This command line action uses a YAML configuration file to configure a run script execution. The inputs/outputs are specified in the YAML file via the `${{ ... }}` notation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a component configuration file that references the conda.yml (above) to create a job and initialize execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components\\data_prep\\cli_job.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n",
    "code: .\n",
    "command: >-\n",
    "  python data_prep.py \n",
    "  --data ${{inputs.data}} \n",
    "  --test_train_ratio ${{inputs.test_train_ratio}} \n",
    "  --train_data_csv ${{outputs.train_data_csv}}\n",
    "  --test_data_csv ${{outputs.test_data_csv}}\n",
    "inputs:\n",
    "  data: \n",
    "    type: uri_file\n",
    "#    path: azureml:credit-card_csv:2023.10.05.154542\n",
    "  test_train_ratio: \n",
    "    type: number\n",
    "  train_data_csv: \n",
    "    type: string\n",
    "  test_data_csv: \n",
    "    type: string\n",
    "outputs:\n",
    "  train_data_csv:\n",
    "    type: uri_folder\n",
    "    mode: rw_mount\n",
    "  test_data_csv:\n",
    "    type: uri_folder\n",
    "    mode: rw_mount\n",
    "environment: \n",
    "  image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "  conda_file: ../conda.yml #### <--- CONDA.YML\n",
    "compute: azureml:cpu-cluster #### <---- COMPUTE RESOURCE\n",
    "name: data_prep_credit_defaults\n",
    "display_name: Data prep for pipeline\n",
    "experiment_name: data_prep_credit_defaults\n",
    "description: Train a scikit-learn SVM on the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml component create -f components\\data_prep\\cli_job.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create component 2: training (using yaml definition)\n",
    "\n",
    "The second component that you create consumes the training and test data, train a tree based model and return the output model. Use Azure Machine Learning logging capabilities to record and visualize the learning progress.\n",
    "\n",
    "You used the `CommandComponent` class to create your first component. This time you use the yaml definition to define the second component. Each method has its own advantages. A yaml definition can actually be checked-in along the code, and would provide a readable history tracking. The programmatic method using `CommandComponent` can be easier with built-in class documentation and code completion.\n",
    "\n",
    "Create the directory for this component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "train_src_dir"
   },
   "outputs": [],
   "source": [
    "!mkdir .\\components\\train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training script in the directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "train.py"
   },
   "outputs": [],
   "source": [
    "%%writefile .\\components\\train\\train.py\n",
    "import argparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"path: \", path)\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    print(\"Files in path: \", files)\n",
    "\n",
    "\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# Start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data_csv\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data_csv\", type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
    "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    train_df = pd.read_csv(select_first_file(args.train_data_csv))\n",
    "    #train_df = pd.read_csv(args.train_data_csv)\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_train = train_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_train = train_df.values\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    test_df = pd.read_csv(select_first_file(args.test_data_csv))\n",
    "    #test_df = pd.read_csv(args.test_data_csv)\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_test = test_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_test = test_df.values\n",
    "\n",
    "    print(f\"Training with data of shape {X_train.shape}\")\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=clf,\n",
    "        path=os.path.join(args.model, \"trained_model\"),\n",
    "    )\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this training script, once the model is trained, the model file is saved and registered to the workspace. Now you can use the registered model in inferencing endpoints.\n",
    "\n",
    "For the environment of this step, you use one of the built-in (curated) Azure Machine Learning environments. The tag `azureml`, tells the system to use look for the name in curated environments.\n",
    "First, create the *yaml* file describing the component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "train.yml"
   },
   "outputs": [],
   "source": [
    "%%writefile .\\components\\train\\train.yml\n",
    "# <component>\n",
    "name: train_credit_defaults_model\n",
    "display_name: Train Credit Defaults Model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data_csv: \n",
    "    type: uri_folder\n",
    "    mode: ro_mount\n",
    "  test_data_csv: \n",
    "    type: uri_file\n",
    "    mode: ro_mount\n",
    "  learning_rate:\n",
    "    type: number     \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data_csv}} \n",
    "  --test_data ${{inputs.test_data_csv}} \n",
    "  --learning_rate ${{inputs.learning_rate}}\n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create and register the component.  Registering it allows you to re-use it in other pipelines.  Also, anyone else with access to your workspace can use the registered component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml component create -f  .\\components\\train\\train.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline from components\n",
    "\n",
    "Now that both your components are defined and registered, you can start implementing the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you use *input data*, *split ratio* and *registered model name* as input variables. Then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "source": [
    "A YAML configuration file is used to represent the Azure Machine Learning pipeline structure as a directed acyclic graph (DAG). In the YAML configuration, we can specify the pipeline description and default resources like compute and storage.  Similar to components, pipelines can have inputs and output. You can then create multiple instances of a single pipeline with different inputs.\n",
    "\n",
    "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline.yml file to describe the structure of the pipeline DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\components\\pipeline.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\n",
    "type: pipeline\n",
    "\n",
    "display_name: 04_pipeline_with_data_007\n",
    "description: Pipeline with 2 component jobs with data dependencies\n",
    "\n",
    "settings:\n",
    "  default_compute: azureml:cpu-cluster\n",
    "\n",
    "outputs:\n",
    "  final_model:\n",
    "    mode: upload\n",
    "\n",
    "jobs:\n",
    "  component_data_prep:\n",
    "    type: command\n",
    "    inputs:\n",
    "      data: \n",
    "        type: uri_file\n",
    "        path: azureml:credit-card_csv:2023.10.05.154542\n",
    "      test_train_ratio: 0.25\n",
    "      train_data_csv: train \n",
    "      test_data_csv: test\n",
    "    outputs:\n",
    "      train_data_csv: \n",
    "        mode: upload\n",
    "      test_data_csv: \n",
    "        mode: upload\n",
    "    code: data_prep\n",
    "    environment: \n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "      conda_file: conda.yml #### <--- CONDA.YML\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python data_prep.py \n",
    "      --data ${{inputs.data}} \n",
    "      --test_train_ratio ${{inputs.test_train_ratio}} \n",
    "      --train_data_csv ${{outputs.train_data_csv}}\n",
    "      --test_data_csv ${{outputs.test_data_csv}}\n",
    "      \n",
    "  component_train:\n",
    "    type: command\n",
    "    inputs:\n",
    "      train_data_csv: ${{parent.jobs.component_data_prep.outputs.train_data_csv}}\n",
    "      test_data_csv: ${{parent.jobs.component_data_prep.outputs.test_data_csv}}\n",
    "      learning_rate: 0.2\n",
    "      registered_model_name: credit_default_model\n",
    "    outputs:\n",
    "      model: ${{parent.outputs.final_model}}\n",
    "    code: train\n",
    "    environment: \n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "      conda_file: data_prep/conda.yml #### <--- CONDA.YML\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python train.py \n",
    "      --train_data ${{inputs.train_data_csv}} \n",
    "      --test_data ${{inputs.test_data_csv}} \n",
    "      --learning_rate ${{inputs.learning_rate}}\n",
    "      --registered_model_name ${{inputs.registered_model_name}} \n",
    "      --model ${{outputs.model}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create --file .\\components\\pipeline.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your pipeline definition to instantiate a pipeline with your dataset, split rate of choice and the name you picked for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track the progress of your pipeline, by using the link generated in the previous cell. When you first select this link, you may see that the pipeline is still running. Once it's complete, you can examine each component's results.\n",
    "\n",
    "Double-click the **Train Credit Defaults Model** component. \n",
    "\n",
    "There are two important results you'll want to see about training:\n",
    "\n",
    "* View your logs:\n",
    "    1. Select the **Outputs+logs** tab.\n",
    "    1. Open the folders to `user_logs` > `std_log.txt`\n",
    "    This section shows the script run stdout.\n",
    "    ![Screenshot of std_log.txt.](media/user-logs.jpg)\n",
    "\n",
    "* View your metrics: Select the **Metrics** tab.  This section shows different logged metrics. In this example. mlflow `autologging`, has automatically logged the training metrics.\n",
    "    \n",
    "    ![Screenshot shows logged metrics.txt.](./media/metrics.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model as an online endpoint\n",
    "To learn how to deploy your model to an online endpoint, see [Deploy a model as an online endpoint tutorial](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Learn how to [Schedule machine learning pipeline jobs](https://learn.microsoft.com/azure/machine-learning/how-to-schedule-pipeline-job)"
   ]
  }
 ],
 "metadata": {
  "categories": [
   "SDK v2",
   "tutorials",
   "get-started-notebooks"
  ],
  "description": {
   "description": "Create production ML pipelines with Python SDK v2 in a Jupyter notebook"
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "aigbb-aml-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
