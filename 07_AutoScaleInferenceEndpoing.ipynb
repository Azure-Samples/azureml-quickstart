{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create handle to workspace\n",
    "\n",
    "Before we dive in the code, you need a way to reference your workspace. You'll create `ml_client` for a handle to the workspace.  You'll then use `ml_client` to manage resources and jobs.\n",
    "\n",
    "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find these values:\n",
    "\n",
    "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
    "1. Copy the value for workspace, resource group and subscription ID into the code.\n",
    "1. You'll need to copy one value, close the area and paste, then come back for the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "gather": {
     "logged": 1671511884101
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "# # Get a handle to the workspace\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = os.environ.get('SUBSCRIPTION_ID'),\n",
    "    resource_group_name = os.environ.get('RESOURCE_GROUP_NAME'),\n",
    "    workspace_name = os.environ.get('WORKSPACE_NAME'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aigbb-aml-bootcamp\n",
      "aigbb-aml-bootcamp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('WORKSPACE_NAME'))\n",
    "print(os.environ.get('RESOURCE_GROUP_NAME'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define the endpoint, using the `ManagedOnlineEndpoint` class.\n",
    "\n",
    "\n",
    "\n",
    "> [!TIP]\n",
    "> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](https://learn.microsoft.com/azure/machine-learning/how-to-authenticate-online-endpoint).\n",
    "> * Optionally, you can add a description and tags to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_endpoint_name = \"deployment-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, KubernetesOnlineEndpoint\n",
    "\n",
    "online_endpoint_name = \"deployment-test\"\n",
    "\n",
    "# define an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is an online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the online endpoint\n",
    "# expect the endpoint to take approximately 2 minutes.\n",
    "\n",
    "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint \"geico-deployment-test\" with provisioning state \"Succeeded\" is retrieved\n"
     ]
    }
   ],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(\n",
    "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineDeployment, KubernetesOnlineDeployment\n",
    "from azure.ai.ml.entities import TargetUtilizationScaleSettings\n",
    "\n",
    "# Choose the latest version of our registered model for deployment\n",
    "model = ml_client.models.get(name=\"credit_defaults_model\", label=\"latest\")\n",
    "\n",
    "# define an online deployment\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name = online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `MLClient` created earlier, we'll now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint geico-deployment-test exists\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://geico-deployment-test.eastus.inference.ml.azure.com/score', 'openapi_uri': 'https://geico-deployment-test.eastus.inference.ml.azure.com/swagger.json', 'name': 'geico-deployment-test', 'description': 'this is an online endpoint', 'tags': {'training_dataset': 'geico'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourcegroups/aigbb-aml-bootcamp/providers/microsoft.machinelearningservices/workspaces/aigbb-aml-bootcamp/onlineendpoints/geico-deployment-test', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:79884ed9-220e-45cf-b7fe-bc488567ee26:cae7a5ce-901b-4744-8ca1-db03739f8aa6?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\antonslutsky\\\\Dev2\\\\aigbb-aml-bootcamp', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000027939FACD90>, 'auth_mode': 'key', 'location': 'eastus', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x0000027939A5FF10>, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the online deployment\n",
    "blue_deployment = ml_client.online_deployments.begin_create_or_update(\n",
    "    blue_deployment\n",
    ").result()\n",
    "\n",
    "# blue deployment takes 100% traffic\n",
    "# expect the deployment to take approximately 8 to 10 minutes.\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\n"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment show -e \"deployment-test\" -n \"blue\" -o tsv --query \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\n"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment show -e deployment-test -n blue -o tsv --query \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"enabled\": true,\n",
      "  \"id\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/microsoft.insights/autoscalesettings/autoscale-geico-deployment-test\",\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"autoscale-geico-deployment-test\",\n",
      "  \"notifications\": [\n",
      "    {\n",
      "      \"email\": {\n",
      "        \"customEmails\": [],\n",
      "        \"sendToSubscriptionAdministrator\": false,\n",
      "        \"sendToSubscriptionCoAdministrators\": false\n",
      "      },\n",
      "      \"operation\": \"Scale\",\n",
      "      \"webhooks\": []\n",
      "    }\n",
      "  ],\n",
      "  \"predictiveAutoscalePolicy\": {\n",
      "    \"scaleMode\": \"Disabled\"\n",
      "  },\n",
      "  \"profiles\": [\n",
      "    {\n",
      "      \"capacity\": {\n",
      "        \"default\": \"1\",\n",
      "        \"maximum\": \"5\",\n",
      "        \"minimum\": \"1\"\n",
      "      },\n",
      "      \"name\": \"default\",\n",
      "      \"rules\": []\n",
      "    }\n",
      "  ],\n",
      "  \"resourceGroup\": \"aigbb-aml-bootcamp\",\n",
      "  \"tags\": {},\n",
      "  \"targetResourceUri\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\",\n",
      "  \"type\": \"Microsoft.Insights/autoscaleSettings\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Follow up with `az monitor autoscale rule create` to add scaling rules.\n"
     ]
    }
   ],
   "source": [
    "!az monitor autoscale create \\\n",
    "  --name autoscale-deployment-test \\\n",
    "  --resource \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/deployment-test/deployments/blue\" \\\n",
    "  --min-count 1 --max-count 5 --count 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Profile 'default' has rules to scale out but none to scale in. Recommend creating at least 1 scale in rule.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  \"metricTrigger\": {\n",
      "    \"dividePerInstance\": false,\n",
      "    \"metricName\": \"CpuUtilizationPercentage\",\n",
      "    \"metricNamespace\": \"\",\n",
      "    \"metricResourceUri\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\",\n",
      "    \"operator\": \"GreaterThan\",\n",
      "    \"statistic\": \"Average\",\n",
      "    \"threshold\": 50.0,\n",
      "    \"timeAggregation\": \"Average\",\n",
      "    \"timeGrain\": \"PT1M\",\n",
      "    \"timeWindow\": \"PT5M\"\n",
      "  },\n",
      "  \"scaleAction\": {\n",
      "    \"cooldown\": \"PT5M\",\n",
      "    \"direction\": \"Increase\",\n",
      "    \"type\": \"ChangeCount\",\n",
      "    \"value\": \"2\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az monitor autoscale rule create \\\n",
    "  --autoscale-name autoscale-deployment-test \\\n",
    "  --condition \"CpuUtilizationPercentage > 50 avg 5m\" \\\n",
    "  --scale out 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metricTrigger\": {\n",
      "    \"dividePerInstance\": false,\n",
      "    \"metricName\": \"CpuUtilizationPercentage\",\n",
      "    \"metricNamespace\": \"\",\n",
      "    \"metricResourceUri\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\",\n",
      "    \"operator\": \"LessThan\",\n",
      "    \"statistic\": \"Average\",\n",
      "    \"threshold\": 25.0,\n",
      "    \"timeAggregation\": \"Average\",\n",
      "    \"timeGrain\": \"PT1M\",\n",
      "    \"timeWindow\": \"PT5M\"\n",
      "  },\n",
      "  \"scaleAction\": {\n",
      "    \"cooldown\": \"PT5M\",\n",
      "    \"direction\": \"Decrease\",\n",
      "    \"type\": \"ChangeCount\",\n",
      "    \"value\": \"1\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az monitor autoscale rule create \\\n",
    "  --autoscale-name autoscale-deployment-test \\\n",
    "  --condition \"CpuUtilizationPercentage < 25 avg 5m\" \\\n",
    "  --scale in 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"index\": 0,\n",
      "    \"metricTrigger\": {\n",
      "      \"dividePerInstance\": false,\n",
      "      \"metricName\": \"CpuUtilizationPercentage\",\n",
      "      \"metricNamespace\": \"\",\n",
      "      \"metricResourceUri\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\",\n",
      "      \"operator\": \"GreaterThan\",\n",
      "      \"statistic\": \"Average\",\n",
      "      \"threshold\": 50.0,\n",
      "      \"timeAggregation\": \"Average\",\n",
      "      \"timeGrain\": \"PT1M\",\n",
      "      \"timeWindow\": \"PT5M\"\n",
      "    },\n",
      "    \"scaleAction\": {\n",
      "      \"cooldown\": \"PT5M\",\n",
      "      \"direction\": \"Increase\",\n",
      "      \"type\": \"ChangeCount\",\n",
      "      \"value\": \"2\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"index\": 1,\n",
      "    \"metricTrigger\": {\n",
      "      \"dividePerInstance\": false,\n",
      "      \"metricName\": \"CpuUtilizationPercentage\",\n",
      "      \"metricNamespace\": \"\",\n",
      "      \"metricResourceUri\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\",\n",
      "      \"operator\": \"LessThan\",\n",
      "      \"statistic\": \"Average\",\n",
      "      \"threshold\": 25.0,\n",
      "      \"timeAggregation\": \"Average\",\n",
      "      \"timeGrain\": \"PT1M\",\n",
      "      \"timeWindow\": \"PT5M\"\n",
      "    },\n",
      "    \"scaleAction\": {\n",
      "      \"cooldown\": \"PT5M\",\n",
      "      \"direction\": \"Decrease\",\n",
      "      \"type\": \"ChangeCount\",\n",
      "      \"value\": \"1\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!az monitor autoscale rule list --autoscale-name autoscale-deployment-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az monitor autoscale rule delete --autoscale-name autoscale-deployment-test --index=*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metricTrigger\": {\n",
      "    \"dividePerInstance\": false,\n",
      "    \"metricName\": \"CpuUtilizationPercentage\",\n",
      "    \"metricNamespace\": \"\",\n",
      "    \"metricResourceUri\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\",\n",
      "    \"operator\": \"GreaterThan\",\n",
      "    \"statistic\": \"Average\",\n",
      "    \"threshold\": 30.0,\n",
      "    \"timeAggregation\": \"Average\",\n",
      "    \"timeGrain\": \"PT1M\",\n",
      "    \"timeWindow\": \"PT5M\"\n",
      "  },\n",
      "  \"scaleAction\": {\n",
      "    \"cooldown\": \"PT5M\",\n",
      "    \"direction\": \"Increase\",\n",
      "    \"type\": \"ChangeCount\",\n",
      "    \"value\": \"2\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Profile 'default' has rules to scale out but none to scale in. Recommend creating at least 1 scale in rule.\n"
     ]
    }
   ],
   "source": [
    "!az monitor autoscale rule create \\\n",
    "  --autoscale-name autoscale-deployment-test \\\n",
    "  --condition \"CpuUtilizationPercentage > 30 avg 5m\" \\\n",
    "  --scale out 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metricTrigger\": {\n",
      "    \"dividePerInstance\": false,\n",
      "    \"metricName\": \"CpuUtilizationPercentage\",\n",
      "    \"metricNamespace\": \"\",\n",
      "    \"metricResourceUri\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/aigbb-aml-bootcamp/providers/Microsoft.MachineLearningServices/workspaces/aigbb-aml-bootcamp/onlineEndpoints/geico-deployment-test/deployments/blue\",\n",
      "    \"operator\": \"LessThan\",\n",
      "    \"statistic\": \"Average\",\n",
      "    \"threshold\": 15.0,\n",
      "    \"timeAggregation\": \"Average\",\n",
      "    \"timeGrain\": \"PT1M\",\n",
      "    \"timeWindow\": \"PT5M\"\n",
      "  },\n",
      "  \"scaleAction\": {\n",
      "    \"cooldown\": \"PT5M\",\n",
      "    \"direction\": \"Decrease\",\n",
      "    \"type\": \"ChangeCount\",\n",
      "    \"value\": \"1\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az monitor autoscale rule create \\\n",
    "  --autoscale-name autoscale-deployment-test \\\n",
    "  --condition \"CpuUtilizationPercentage < 15 avg 5m\" \\\n",
    "  --scale in 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the status of the endpoint\n",
    "You can check the status of the endpoint to see whether the model was deployed without error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: geico-deployment-test\n",
      "Status: Succeeded\n",
      "Description: this is an online endpoint\n"
     ]
    }
   ],
   "source": [
    "# return an object that contains metadata for the endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# print a selection of the endpoint's metadata\n",
    "print(\n",
    "    f\"Name: {endpoint.name}\\nStatus: {endpoint.provisioning_state}\\nDescription: {endpoint.description}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue': 100}\n",
      "https://credit-endpoint-f7daf59c.eastus.inference.ml.azure.com/score\n"
     ]
    }
   ],
   "source": [
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint with sample data\n",
    "\n",
    "Now that the model is deployed to the endpoint, you can run inference with it. Let's create a sample request file following the design expected in the run method in the scoring script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store the sample request file.\n",
    "deploy_dir = \"./deploy\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the file in the deploy directory. The cell below uses IPython magic to write the file into the directory you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\n",
    "  \"input_data\": {\n",
    "    \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
    "    \"index\": [0, 1],\n",
    "    \"data\": [\n",
    "            [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
    "            [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
    "            ]\n",
    "                }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `MLClient` created earlier, we'll get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n",
    "\n",
    "* `endpoint_name` - Name of the endpoint\n",
    "* `request_file` - File with request data\n",
    "* `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We'll test the blue deployment with the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 0]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the blue deployment with the sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get logs of the deployment\n",
    "Check the logs to see whether the endpoint/deployment were invoked successfully\n",
    "If you face errors, see [Troubleshooting online endpoints deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints?tabs=cli)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance status:\n",
      "SystemSetup: Succeeded\n",
      "UserContainerImagePull: Succeeded\n",
      "ModelDownload: Succeeded\n",
      "UserContainerStart: Succeeded\n",
      "\n",
      "Container events:\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:06.805313Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:09.710981Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:16.805237Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:19.710791Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:26.805435Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:29.710829Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:36.80522Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:39.710829Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:46.805261Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:49.71071Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:56.80532Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:19:59.710865Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:20:06.805202Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:20:09.710737Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:20:16.805417Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:20:19.710982Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:20:26.805345Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-09-06T21:20:29.710751Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-09-06T21:20:36.805654Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\n",
      "Kind: Pod, Name: ContainerReady, Type: Normal, Time: 2023-09-06T21:20:43.95521617Z, Message: Container is ready\n",
      "\n",
      "Container logs:\n",
      "Health Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.8.4.1\n",
      "CORS for the specified origins: None\n",
      "Create dedicated endpoint for health: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "2023-09-06 21:20:39,890 I [699] azmlinfsrv - AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\n",
      "Initializing logger\n",
      "2023-09-06 21:20:39,892 I [699] azmlinfsrv - Starting up app insights client\n",
      "WARNING:entry_module:No signature information provided for model. If no sample information was provided with the model the deployment's swagger will not include input and output schema and typing information.For more information, please see: https://aka.ms/aml-mlflow-deploy.\n",
      "2023-09-06 21:20:41,199 I [699] azmlinfsrv.user_script - Found user script at /var/mlflow_resources/mlflow_score_script.py\n",
      "2023-09-06 21:20:41,199 I [699] azmlinfsrv.user_script - run() is decorated with @input_schema. Server will invoke it with the following arguments: input_data.\n",
      "2023-09-06 21:20:41,199 I [699] azmlinfsrv.user_script - Invoking user's init function\n",
      "2023-09-06 21:20:41,200 | mdc | INFO | init data collector\n",
      "2023-09-06 21:20:41,200 | mdc | INFO | mdc enabled: False\n",
      "2023-09-06 21:20:41,200 | mdc | INFO | mdc sample rate: 100\n",
      "2023-09-06 21:20:41,200 | mdc | WARNING | data collector is not enabled for model default\n",
      "2023-09-06 21:20:41,201 | mdc | INFO | init data collector\n",
      "2023-09-06 21:20:41,201 | mdc | INFO | mdc enabled: False\n",
      "2023-09-06 21:20:41,201 | mdc | INFO | mdc sample rate: 100\n",
      "2023-09-06 21:20:41,201 | mdc | WARNING | data collector is not enabled for model default\n",
      "2023-09-06 21:20:41,202 I [699] azmlinfsrv.user_script - Users's init has completed successfully\n",
      "2023-09-06 21:20:41,202 I [699] azmlinfsrv.swagger - Swaggers are prepared for the following versions: [2, 3, 3.1].\n",
      "2023-09-06 21:20:41,202 I [699] azmlinfsrv - Scoring timeout is set to 3600000\n",
      "2023-09-06 21:20:41,203 I [699] azmlinfsrv - Worker with pid 699 ready for serving traffic\n",
      "2023-09-06 21:20:41,206 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:20:41 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:20:46,808 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:20:46 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:20:49,712 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:20:49 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:20:56,806 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:20:56 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:20:59,711 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:20:59 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:06,806 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:06 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:09,711 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:09 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:16,806 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:16 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:19,711 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:19 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:26,806 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:26 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:29,712 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:29 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:36,806 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:36 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:39,712 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:39 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:46,806 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:46 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:49,711 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:49 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:56,806 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:56 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n",
      "2023-09-06 21:21:59,295 I [699] azmlinfsrv - POST /score 200 8.080ms 6\n",
      "2023-09-06 21:21:59,295 I [699] gunicorn.access - 127.0.0.1 - - [06/Sep/2023:21:21:59 +0000] \"POST /score HTTP/1.0\" 200 6 \"-\" \"azure-ai-ml/1.5.0 azsdk-python-core/1.29.3 Python/3.8.17 (Linux-5.15.0-1040-azure-x86_64-with-glibc2.17)\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs = ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
    ")\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a second deployment \n",
    "Deploy the model as a second deployment called `green`. In practice, you can create several deployments and compare their performance. These deployments could use a different version of the same model, a completely different model, or a more powerful compute instance. In our example, you'll deploy the same model version using a more powerful compute instance that could potentially improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the model to deploy. Here we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "# define an online deployment using a more powerful instance type\n",
    "green_deployment = ManagedOnlineDeployment(\n",
    "    name=\"green\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_F4s_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "# create the online deployment\n",
    "# expect the deployment to take approximately 8 to 10 minutes\n",
    "green_deployment = ml_client.online_deployments.begin_create_or_update(\n",
    "    green_deployment\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale deployment to handle more traffic\n",
    "\n",
    "Using the `MLClient` created earlier, we'll get a handle to the `green` deployment. The deployment can be scaled by increasing or decreasing the `instance_count`.\n",
    "\n",
    "In the following code, you'll increase the VM instance manually. However, note that it is also possible to autoscale online endpoints. Autoscale automatically runs the right amount of resources to handle the load on your application. Managed online endpoints support autoscaling through integration with the Azure monitor autoscale feature. To configure autoscaling, see [autoscale online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-autoscale-endpoints?tabs=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update definition of the deployment\n",
    "green_deployment.instance_count = 2\n",
    "\n",
    "# update the deployment\n",
    "# expect the deployment to take approximately 8 to 10 minutes\n",
    "ml_client.online_deployments.begin_create_or_update(green_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update traffic allocation for deployments\n",
    "You can split production traffic between deployments. You may first want to test the `green` deployment with sample data, just like you did for the `blue` deployment. Once you've tested your green deployment, allocate a small percentage of traffic to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 80, \"green\": 20}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test traffic allocation by invoking the endpoint several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can invoke the endpoint several times\n",
    "for i in range(30):\n",
    "    ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        request_file=\"./deploy/sample-request.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show logs from the `green` deployment to check that there were incoming requests and the model was scored successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = ml_client.online_deployments.get_logs(\n",
    "    name=\"green\", endpoint_name=online_endpoint_name, lines=50\n",
    ")\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View metrics using Azure Monitor\n",
    "You can view various metrics (request numbers, request latency, network bytes, CPU/GPU/Disk/Memory utilization, and more) for an online endpoint and its deployments by following links from the endpoint's **Details** page in the studio. Following these links will take you to the exact metrics page in the Azure portal for the endpoint or deployment.\n",
    "\n",
    "![metrics page 1](./media/deployment-metrics-from-endpoint-details-page.png)\n",
    "\n",
    "\n",
    "If you open the metrics for the online endpoint, you can set up the page to see metrics such as the average request latency as shown in the following figure.\n",
    "\n",
    "![metrics page 2](./media/view-endpoint-metrics-in-azure-portal.png)\n",
    "\n",
    "For more information on how to view online endpoint metrics, see [Monitor online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-monitor-online-endpoints#metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send all traffic to the new deployment\n",
    "Once you're fully satisfied with your `green` deployment, switch all traffic to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 0, \"green\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the old deployment\n",
    "Remove the old (blue) deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_delete(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "If you aren't going use the endpoint and deployment after completing this tutorial, you should delete them.\n",
    "\n",
    "> [!NOTE]\n",
    "> Expect the complete deletion to take approximately 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name).result()"
   ]
  }
 ],
 "metadata": {
  "categories": [
   "SDK v2",
   "tutorials",
   "get-started-notebooks"
  ],
  "description": {
   "description": "Learn to deploy a model to an online endpoint, using Azure Machine Learning Python SDK v2."
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "aigbb-aml-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
