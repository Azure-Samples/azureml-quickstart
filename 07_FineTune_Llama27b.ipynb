{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml model show --name Llama-2-7b --label latest --registry-name azureml-meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating json from Arrow format:   0%|          | 0/16 [00:00<?, ?ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 16/16 [00:00<00:00, 222.79ba/s]\n",
      "\n",
      "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 140.07ba/s]\n",
      "\n",
      "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 158.55ba/s]\n"
     ]
    }
   ],
   "source": [
    "!python ./src/download_dataset.py --download_dir ./data/emotions-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"$schema\": \"https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json\",\n",
      "  \"creation_context\": {\n",
      "    \"created_at\": \"2024-02-15T10:25:49.504708+00:00\",\n",
      "    \"created_by\": \"Microsoft\",\n",
      "    \"created_by_type\": \"User\",\n",
      "    \"last_modified_at\": \"2024-02-15T10:25:49.504708+00:00\",\n",
      "    \"last_modified_by\": \"Microsoft\",\n",
      "    \"last_modified_by_type\": \"User\"\n",
      "  },\n",
      "  \"description\": \"Pipeline component to finetune Hugging Face pretrained models for text classification task. The component supports optimizations such as LoRA, Deepspeed and ONNXRuntime for performance enhancement. See [docs](https://aka.ms/azureml/components/text_classification_pipeline) to learn more.\",\n",
      "  \"display_name\": \"Text Classification Pipeline\",\n",
      "  \"id\": \"azureml://registries/azureml/components/text_classification_pipeline/versions/0.0.34\",\n",
      "  \"inputs\": {\n",
      "    \"adam_beta1\": {\n",
      "      \"default\": \"0.9\",\n",
      "      \"description\": \"beta1 hyperparameter for the AdamW optimizer\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"adam_beta2\": {\n",
      "      \"default\": \"0.999\",\n",
      "      \"description\": \"beta2 hyperparameter for the AdamW optimizer\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"adam_epsilon\": {\n",
      "      \"default\": \"1e-08\",\n",
      "      \"description\": \"epsilon hyperparameter for the AdamW optimizer\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"apply_deepspeed\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"If set to true, will enable deepspeed for training\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"apply_early_stopping\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"If set to \\\"true\\\", early stopping is enabled.\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"apply_lora\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"If \\\"true\\\" enables lora.\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"apply_ort\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"If set to true, will use the ONNXRunTime training\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"auto_find_batch_size\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"If set to \\\"true\\\" and if the provided 'per_device_train_batch_size' goes into Out Of Memory (OOM) auto_find_batch_size will find the correct batch size by iteratively reducing batch size by a factor of 2 till the OOM is fixed\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"batch_size\": {\n",
      "      \"default\": \"1000\",\n",
      "      \"description\": \"Number of examples to batch before calling the tokenization function\",\n",
      "      \"min\": \"1\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"compute_finetune\": {\n",
      "      \"default\": \"serverless\",\n",
      "      \"description\": \"compute to be used for finetune eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \\\\ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"compute_model_evaluation\": {\n",
      "      \"default\": \"serverless\",\n",
      "      \"description\": \"compute to be used for model_eavaluation eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \\\\ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"compute_model_import\": {\n",
      "      \"default\": \"serverless\",\n",
      "      \"description\": \"compute to be used for model_import eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \\\\ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"compute_preprocess\": {\n",
      "      \"default\": \"serverless\",\n",
      "      \"description\": \"compute to be used for preprocess eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \\\\ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"dataloader_num_workers\": {\n",
      "      \"default\": \"0\",\n",
      "      \"description\": \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"deepspeed\": {\n",
      "      \"description\": \"Deepspeed config to be used for finetuning. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"deepspeed_stage\": {\n",
      "      \"default\": \"2\",\n",
      "      \"description\": \"This parameter configures which DEFAULT deepspeed config to be used - stage2 or stage3. The default choice is stage2. Note that, this parameter is ONLY applicable when user doesn't pass any config information via deepspeed port.\",\n",
      "      \"enum\": [\n",
      "        \"2\",\n",
      "        \"3\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"early_stopping_patience\": {\n",
      "      \"default\": \"1\",\n",
      "      \"description\": \"Stop training when the metric specified through _metric_for_best_model_ worsens for _early_stopping_patience_ evaluation calls.This value is only valid if _apply_early_stopping_ is set to true.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"early_stopping_threshold\": {\n",
      "      \"default\": \"0.0\",\n",
      "      \"description\": \"Denotes how much the specified metric must improve to satisfy early stopping conditions. This value is only valid if _apply_early_stopping_ is set to true.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"enable_full_determinism\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"Ensure reproducible behavior during distributed training. Check this link https://pytorch.org/docs/stable/notes/randomness.html for more details.\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"eval_accumulation_steps\": {\n",
      "      \"default\": \"-1\",\n",
      "      \"description\": \"Number of predictions steps to accumulate before moving the tensors to the CPU, will be passed as None if set to -1\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"eval_steps\": {\n",
      "      \"default\": \"500\",\n",
      "      \"description\": \"Number of update steps between two evals if evaluation_strategy='steps'\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"evaluation_config\": {\n",
      "      \"description\": \"Additional parameters for Computing Metrics. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"evaluation_config_params\": {\n",
      "      \"description\": \"Additional parameters as JSON serielized string\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"evaluation_steps_interval\": {\n",
      "      \"default\": \"0.0\",\n",
      "      \"description\": \"The evaluation steps in fraction of an epoch steps to adopt during training. Overwrites eval_steps if not 0.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"evaluation_strategy\": {\n",
      "      \"default\": \"epoch\",\n",
      "      \"description\": \"The evaluation strategy to adopt during training. If set to \\\"steps\\\", either the `evaluation_steps_interval` or `eval_steps` needs to be specified, which helps to determine the step at which the model evaluation needs to be computed else evaluation happens at end of each epoch.\",\n",
      "      \"enum\": [\n",
      "        \"epoch\",\n",
      "        \"steps\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"gradient_accumulation_steps\": {\n",
      "      \"default\": \"1\",\n",
      "      \"description\": \"Number of updates steps to accumulate the gradients for, before performing a backward/update pass\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"huggingface_id\": {\n",
      "      \"description\": \"The string can be any valid Hugging Face id from the [Hugging Face models webpage](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads). Models from Hugging Face are subject to third party license terms available on the Hugging Face model details page. It is your responsibility to comply with the model's license terms. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"ignore_mismatched_sizes\": {\n",
      "      \"default\": \"true\",\n",
      "      \"description\": \"Not setting this flag will raise an error if some of the weights from the checkpoint do not have the same size as the weights of the model.\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"instance_type_finetune\": {\n",
      "      \"default\": \"Standard_nc24rs_v3\",\n",
      "      \"description\": \"Instance type to be used for finetune component in case of serverless compute, eg. standard_nc24rs_v3. The parameter compute_finetune must be set to 'serverless' for instance_type to be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"instance_type_model_evaluation\": {\n",
      "      \"default\": \"Standard_nc24rs_v3\",\n",
      "      \"description\": \"Instance type to be used for model_evaluation components in case of serverless compute, eg. standard_nc24rs_v3. The parameter compute_model_evaluation must be set to 'serverless' for instance_type to be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"instance_type_model_import\": {\n",
      "      \"default\": \"Standard_d12_v2\",\n",
      "      \"description\": \"Instance type to be used for model_import component in case of serverless compute, eg. standard_d12_v2. The parameter compute_model_import must be set to 'serverless' for instance_type to be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"instance_type_preprocess\": {\n",
      "      \"default\": \"Standard_d12_v2\",\n",
      "      \"description\": \"Instance type to be used for preprocess component in case of serverless compute, eg. standard_d12_v2. The parameter compute_preprocess must be set to 'serverless' for instance_type to be used\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"label_key\": {\n",
      "      \"description\": \"label key in each example line. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": false,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"learning_rate\": {\n",
      "      \"default\": \"2e-05\",\n",
      "      \"description\": \"Start learning rate used for training.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"logging_steps\": {\n",
      "      \"default\": \"500\",\n",
      "      \"description\": \"Number of update steps between two logs if logging_strategy='steps'\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"logging_strategy\": {\n",
      "      \"default\": \"epoch\",\n",
      "      \"description\": \"The logging strategy to adopt during training. If set to \\\"steps\\\", the `logging_steps` will decide the frequency of logging else logging happens at the end of epoch..\",\n",
      "      \"enum\": [\n",
      "        \"epoch\",\n",
      "        \"steps\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"lora_alpha\": {\n",
      "      \"default\": \"128\",\n",
      "      \"description\": \"alpha attention parameter for lora.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"lora_dropout\": {\n",
      "      \"default\": \"0.0\",\n",
      "      \"description\": \"lora dropout value\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"lora_r\": {\n",
      "      \"default\": \"8\",\n",
      "      \"description\": \"lora dimension\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"lr_scheduler_type\": {\n",
      "      \"default\": \"linear\",\n",
      "      \"description\": \"learning rate scheduler to use.\",\n",
      "      \"enum\": [\n",
      "        \"linear\",\n",
      "        \"cosine\",\n",
      "        \"cosine_with_restarts\",\n",
      "        \"polynomial\",\n",
      "        \"constant\",\n",
      "        \"constant_with_warmup\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"max_grad_norm\": {\n",
      "      \"default\": \"1.0\",\n",
      "      \"description\": \"Maximum gradient norm (for gradient clipping)\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"max_seq_length\": {\n",
      "      \"default\": \"-1\",\n",
      "      \"description\": \"Controls the maximum length to use when pad_to_max_length parameter is set to `true`. Default is -1 which means the padding is done up to the model's max length. Else will be padded to `max_seq_length`.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"max_steps\": {\n",
      "      \"default\": \"-1\",\n",
      "      \"description\": \"If set to a positive number, the total number of training steps to perform. Overrides 'epochs'. In case of using a finite iterable dataset the training may stop before reaching the set number of steps when all data is exhausted.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"merge_lora_weights\": {\n",
      "      \"default\": \"true\",\n",
      "      \"description\": \"If \\\"true\\\", the lora weights are merged with the base Hugging Face model weights before saving.\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"metric_for_best_model\": {\n",
      "      \"default\": \"loss\",\n",
      "      \"description\": \"metric to use to compare two different model checkpoints\",\n",
      "      \"enum\": [\n",
      "        \"loss\",\n",
      "        \"f1_macro\",\n",
      "        \"mcc\",\n",
      "        \"accuracy\",\n",
      "        \"precision_macro\",\n",
      "        \"recall_macro\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"mlflow_model_path\": {\n",
      "      \"description\": \"MLflow model asset path. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"mlflow_model\"\n",
      "    },\n",
      "    \"num_nodes_finetune\": {\n",
      "      \"default\": \"1\",\n",
      "      \"description\": \"number of nodes to be used for finetuning (used for distributed training)\",\n",
      "      \"min\": \"1\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"num_train_epochs\": {\n",
      "      \"default\": \"1\",\n",
      "      \"description\": \"Number of epochs to run for finetune.\",\n",
      "      \"min\": \"1\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"number_of_gpu_to_use_finetuning\": {\n",
      "      \"default\": \"1\",\n",
      "      \"description\": \"number of gpus to be used per node for finetuning, should be equal to number of gpu per node in the compute SKU used for finetune\",\n",
      "      \"min\": \"1\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"optim\": {\n",
      "      \"default\": \"adamw_hf\",\n",
      "      \"description\": \"Optimizer to be used while training\",\n",
      "      \"enum\": [\n",
      "        \"adamw_hf\",\n",
      "        \"adamw_torch\",\n",
      "        \"adafactor\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"pad_to_max_length\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"If set to True, the returned sequences will be padded according to the model's padding side and padding index, up to their `max_seq_length`. If no `max_seq_length` is specified, the padding is done up to the model's max length.\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"per_device_eval_batch_size\": {\n",
      "      \"default\": \"1\",\n",
      "      \"description\": \"Per gpu batch size used for validation. The default value is 1. The effective validation batch size is _per_device_eval_batch_size_ * _num_gpus_ * _num_nodes_.\",\n",
      "      \"min\": \"1\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"per_device_train_batch_size\": {\n",
      "      \"default\": \"1\",\n",
      "      \"description\": \"Per gpu batch size used for training. The effective training batch size is _per_device_train_batch_size_ * _num_gpus_ * _num_nodes_.\",\n",
      "      \"min\": \"1\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"precision\": {\n",
      "      \"default\": \"32\",\n",
      "      \"description\": \"Apply mixed precision training. This can reduce memory footprint by performing operations in half-precision.\",\n",
      "      \"enum\": [\n",
      "        \"32\",\n",
      "        \"16\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"pytorch_model_path\": {\n",
      "      \"description\": \"Pytorch model asset path. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"custom_model\"\n",
      "    },\n",
      "    \"resume_from_checkpoint\": {\n",
      "      \"default\": \"false\",\n",
      "      \"description\": \"If set to \\\"true\\\", resumes the training from last saved checkpoint. Along with loading the saved weights, saved optimizer, scheduler and random states will be loaded if exist. The default value is \\\"false\\\"\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"save_total_limit\": {\n",
      "      \"default\": \"-1\",\n",
      "      \"description\": \"If a positive value is passed, it will limit the total number of checkpoints saved. The value of -1 saves all the checkpoints, otherwise if the number of checkpoints exceed the _save_total_limit_, the older checkpoints gets deleted.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"seed\": {\n",
      "      \"default\": \"42\",\n",
      "      \"description\": \"Random seed that will be set at the beginning of training\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"sentence1_key\": {\n",
      "      \"description\": \"Key for `sentence1_key` in each example line. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": false,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"sentence2_key\": {\n",
      "      \"description\": \"Key for `sentence2_key` in each example line. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"shm_size_finetune\": {\n",
      "      \"default\": \"5g\",\n",
      "      \"description\": \"Shared memory size to be used for finetune component. It is useful while using Nebula (via DeepSpeed) which uses shared memory to save model and optimizer states.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"task_name\": {\n",
      "      \"default\": \"SingleLabelClassification\",\n",
      "      \"description\": \"Text Classification task type\",\n",
      "      \"enum\": [\n",
      "        \"SingleLabelClassification\"\n",
      "      ],\n",
      "      \"optional\": false,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"test_file_path\": {\n",
      "      \"description\": \"Path to the registered test data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"test_mltable_path\": {\n",
      "      \"description\": \"Path to the registered test data asset in `mltable` format. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"mltable\"\n",
      "    },\n",
      "    \"train_file_path\": {\n",
      "      \"description\": \"Path to the registered training data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"train_mltable_path\": {\n",
      "      \"description\": \"Path to the registered training data asset in `mltable` format. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"mltable\"\n",
      "    },\n",
      "    \"validation_file_path\": {\n",
      "      \"description\": \"Path to the registered validation data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"validation_mltable_path\": {\n",
      "      \"description\": \"Path to the registered validation data asset in `mltable` format. Special characters like \\\\ and ' are invalid in the parameter value.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"mltable\"\n",
      "    },\n",
      "    \"warmup_steps\": {\n",
      "      \"default\": \"0\",\n",
      "      \"description\": \"Number of steps for the learning rate scheduler warmup phase.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"weight_decay\": {\n",
      "      \"default\": \"0.0\",\n",
      "      \"description\": \"Weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"number\"\n",
      "    }\n",
      "  },\n",
      "  \"is_deterministic\": false,\n",
      "  \"name\": \"text_classification_pipeline\",\n",
      "  \"outputs\": {\n",
      "    \"evaluation_result\": {\n",
      "      \"description\": \"Test Data Evaluation Results\",\n",
      "      \"type\": \"uri_folder\"\n",
      "    },\n",
      "    \"mlflow_model_folder\": {\n",
      "      \"description\": \"output folder containing _best_ finetuned model in mlflow format.\",\n",
      "      \"type\": \"mlflow_model\"\n",
      "    },\n",
      "    \"pytorch_model_folder\": {\n",
      "      \"description\": \"output folder containing _best_ model as defined by _metric_for_best_model_. Along with the best model, output folder contains checkpoints saved after every evaluation which is defined by the _evaluation_strategy_. Each checkpoint contains the model weight(s), config, tokenizer, optimzer, scheduler and random number states.\",\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"type\": \"pipeline\",\n",
      "  \"version\": \"0.0.34\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az ml component show --name text_classification_pipeline --label latest --registry-name azureml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharp_snake_r4kfxk65vk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "!az ml job create --file ./dependencies/emotion-detection-pipeline.yml \\\n",
    "  --resource-group antonslutsky-rg --workspace-name gpu-workspace \\\n",
    "  --query name -o tsv --set \\\n",
    "  jobs.text_classification_pipeline.component=\"azureml://registries/azureml/components/text_classification_pipeline/labels/latest\" \\\n",
    "  inputs.compute_model_import=four-nodes \\\n",
    "  inputs.compute_preprocess=four-nodes \\\n",
    "  inputs.compute_finetune=four-nodes \\\n",
    "  inputs.compute_model_evaluation=four-nodes \\\n",
    "  inputs.mlflow_model_path.path=\"azureml://registries/azureml-meta/models/Llama-2-7b/versions/16\" \\\n",
    "  inputs.train_file_path.path=../data/emotions-dataset/train.jsonl \\\n",
    "  inputs.validation_file_path.path=../data/emotions-dataset/validation.jsonl \\\n",
    "  inputs.test_file_path.path=../data/emotions-dataset/test.jsonl \\\n",
    "  inputs.evaluation_config_path.path=./text_classification_config.json \\\n",
    "  inputs.sentence1_key=text \\\n",
    "  inputs.label_key=label_string \\\n",
    "  inputs.number_of_gpu_to_use_finetuning=1 \\\n",
    "  inputs.num_train_epochs=3 \\\n",
    "  inputs.per_device_train_batch_size=1 \\\n",
    "  inputs.per_device_eval_batch_size=1 \\\n",
    "  inputs.learning_rate=2e-5 \\\n",
    "  inputs.metric_for_best_model=f1_macro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
