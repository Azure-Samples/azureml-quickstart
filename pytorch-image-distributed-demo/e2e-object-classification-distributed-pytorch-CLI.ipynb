{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1aadfd2",
   "metadata": {},
   "source": [
    "# Distributed PyTorch Image Classification\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial you should be able to use Azure Machine Learning (AzureML) to:\n",
    "- quickly implement basic commands for data preparation\n",
    "- test and run a multi-node multi-gpu pytorch job\n",
    "- use mlflow to analyze your metrics\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you need:\n",
    "- to have provisioned an AzureML workspace\n",
    "- to have permissions to provision a minimal cpu and gpu cluster or simply use [serverless compute (preview)](https://learn.microsoft.com/azure/machine-learning/how-to-use-serverless-compute?view=azureml-api-2&tabs=python)\n",
    "- to have [installed Azure Machine Learning Python SDK v2](https://github.com/Azure/azureml-examples/blob/sdk-preview/sdk/README.md)\n",
    "\n",
    "**Motivations** - Let's consider the following scenario: we want to explore training different image classifiers on distinct kinds of problems, based on a large public dataset that is available at a given url. This ML pipeline will be future-looking, in particular we want:\n",
    "- **genericity**: to be fairly independent from the data we're ingesting (so that we could switch to internal proprietary data in the future),\n",
    "- **configurability**: to run different versions of that training with simple configuration changes,\n",
    "- **scalability**: to iterate on the pipeline on small sample, then smoothly transition to running at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641f516",
   "metadata": {},
   "source": [
    "### Connect to AzureML\n",
    "\n",
    "Before we dive in the code, we'll need to create an instance of MLClient to connect to Azure ML.\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios.\n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](https://github.com/Azure/azureml-examples/blob/sdk-preview/sdk/jobs/configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252022c6-59d1-46d8-a07d-0671ea3e0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az extension add -n ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d2657-00bb-43c2-9faa-eef6075d1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login --use-device-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28eaf4-349e-4935-afe4-cb795c56a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az configure --defaults workspace=aml-default group=rg_aml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c812b02-d53b-4377-9ea4-c46c3b5ed7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml compute create -n gpu-nc12sv3 --type amlcompute \\\n",
    "    --min-instances 0 \\\n",
    "    --max-instances 4 \\\n",
    "    --size STANDARD_DS3_V2 \\\n",
    "    --idle-time-before-scale-down 1800 \\\n",
    "    --tier Dedicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3ce5e",
   "metadata": {},
   "source": [
    "### Provision the required resources for this notebook (Optional)\n",
    "\n",
    "We'll need 2 clusters for this notebook, a CPU cluster and a GPU cluster. First, let's create a minimal cpu cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481d489",
   "metadata": {},
   "source": [
    "For GPUs, we're creating the cluster below with the smallest VM family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ab207",
   "metadata": {},
   "source": [
    "# 1. Unpack a public image archives with a simple command (no code)\n",
    "\n",
    "To train our classifier, we'll consume the [Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) or the [Places2 dataset](http://places2.csail.mit.edu/download.html). If we were to use this locally, the sequence would be very basic: download a large tar archive, untar and put in different train/validation folders, upload to the cloud for consumption by the training script.\n",
    "\n",
    "We'll do just that, but in the cloud, without too much pain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cca166",
   "metadata": {},
   "source": [
    "## 1.1. Unpack a first small dataset for testing\n",
    "\n",
    "The Azure ML SDK provides `entities` to implement any step of a workflow. In the example below, we create a `CommandJob` with just a shell command. We parameterize this command by using a string template syntax provided by the SDK:\n",
    "\n",
    "> ```\n",
    "> tar xvfm ${{inputs.archive}} --no-same-owner -C ${{outputs.images}}\n",
    "> ```\n",
    "\n",
    "Creating the component just consists in declaring the names of the inputs, outputs, and specifying an environment. For this simple job we'll use a curated environment from AzureML. After that, we'll be able to reuse that component multiple times in our pipeline design.\n",
    "\n",
    "Note: in this job, we're using an input type `uri_file` with a direct url. In this case, Azure ML will download the file from the url and provide it for the job to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc7972-f4bb-42e2-8186-00be695a26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile untar_dogs_job.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n",
    "command: >-\n",
    "  tar xvfm ${{inputs.archive}} --no-same-owner -C ${{outputs.images}}\n",
    "inputs:\n",
    "  archive: \n",
    "    type: uri_file\n",
    "    path: http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
    "outputs:\n",
    "  images:\n",
    "    type: uri_folder\n",
    "    path: azureml://datastores/workspaceblobstore/paths/tutorial-datasets/dogs/\n",
    "    mode: upload\n",
    "environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "compute: azureml:cpu-cluster\n",
    "display_name: untar_dogs\n",
    "experiment_name: pytorch_training_sample\n",
    "description: PyTorch training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ac5b4-5d82-4ca7-9a33-7274a7340b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create -f untar_dogs_job.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f625a5",
   "metadata": {},
   "source": [
    "## 1.2. Unpack a second larger dataset for training [optional]\n",
    "\n",
    "If you'd like to test the distributed training job below with a more complex dataset, the code below will unpack the [Places2 dataset](http://places2.csail.mit.edu/download.html) dataset images, which has 1.8 million images in 365 categories. This will require a larger VM than the one you provisioned earlier. We recommend you provision a [STANDARD_DS12_V2](https://docs.microsoft.com/en-us/azure/virtual-machines/dv2-dsv2-series-memory). The code below will use compute cluster name `cpu-cluster-lg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd192f56-e202-4a9d-be48-0c794b77f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile untar_places2_job.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n",
    "command: >-\n",
    "  tar xvfm ${{inputs.archive}} --no-same-owner -C ${{outputs.valid_images}} places365_standard/val/; tar xvfm ${{inputs.archive}} --no-same-owner -C ${{outputs.train_images}} places365_standard/train/\n",
    "inputs:\n",
    "  archive: \n",
    "    type: uri_file\n",
    "    path: http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar\n",
    "outputs:\n",
    "  train_images:\n",
    "    type: uri_folder\n",
    "    path: azureml://datastores/workspaceblobstore/paths/tutorial-datasets/places2/train/\n",
    "    mode: upload\n",
    "  valid_images:\n",
    "    type: uri_folder\n",
    "    path: azureml://datastores/workspaceblobstore/paths/tutorial-datasets/places2/valid/\n",
    "    mode: upload\n",
    "environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "compute: azureml:gpu-nc12sv3\n",
    "display_name: untar_places2_job\n",
    "experiment_name: pytorch_training_sample\n",
    "description: PyTorch training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51917738-2588-4017-9dbe-548f1f808f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create -f untar_places2_job.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26664b",
   "metadata": {},
   "source": [
    "# 2. Training a distributed gpu job\n",
    "\n",
    "Implementing a distributed pytorch training is complex. Of course in this tutorial we've written one for you, but the point is: it takes time, it takes several iterations, each requiring you to try your code locally, then in the cloud, then try it at scale, until satisfied and then run a full blown production model training. This trial/error process can be made easier if we can create reusable code we can iterate on quickly, and that can be configured to run from small to large scale.\n",
    "\n",
    "So, to develop our training pipeline, we set a couple constraints for ourselves:\n",
    "- we want to minimize the effort to iterate on the pipeline code when porting it in the cloud,\n",
    "- we want to use the same code for small scale and large scale testing\n",
    "- we do not want to manipulate large data locally (ex: download/upload that data could take multiple hours),\n",
    "\n",
    "We've implemented a distributed pytorch training script that we can load as a command job. For this, we've decided to parameterize this job with relevant training arguments (see below).\n",
    "\n",
    "We can now test this code by running it on a smaller dataset in Azure ML. Here, we will use the dogs dataset both for training and validation. Of course, the model will not be valid. But training will be short (8 mins on 2 x STANDARD_NC6 for 1 epoch) to allow us to iterate if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93f792-2d9d-4f1c-9b7a-d5264bd6b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytorch_training_job.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n",
    "code: ./src/pytorch_dl_train/\n",
    "command: >-\n",
    "    python train.py \\\n",
    "        --train_images ${{inputs.train_images}} \\\n",
    "        --valid_images ${{inputs.valid_images}} \\\n",
    "        --batch_size ${{inputs.batch_size}} \\\n",
    "        --num_workers ${{inputs.num_workers}} \\\n",
    "        --prefetch_factor ${{inputs.prefetch_factor}} \\\n",
    "        --model_arch ${{inputs.model_arch}} \\\n",
    "        --model_arch_pretrained ${{inputs.model_arch_pretrained}} \\\n",
    "        --num_epochs ${{inputs.num_epochs}} \\\n",
    "        --learning_rate ${{inputs.learning_rate}} \\\n",
    "        --momentum ${{inputs.momentum}} \\\n",
    "        --register_model_as ${{inputs.register_model_as}} \\\n",
    "        --enable_profiling ${{inputs.enable_profiling}}\n",
    "inputs:\n",
    "  train_images:\n",
    "    type: uri_folder\n",
    "    path: azureml://datastores/workspaceblobstore/paths/tutorial-datasets/dogs/\n",
    "    mode: download\n",
    "  valid_images:\n",
    "    type: uri_folder\n",
    "    path: azureml://datastores/workspaceblobstore/paths/tutorial-datasets/dogs/\n",
    "    mode: download\n",
    "  batch_size: 64\n",
    "  num_workers: 5  # number of cpus for pre-fetching\n",
    "  prefetch_factor: 2  # number of batches fetched in advance\n",
    "  model_arch: \"resnet18\"\n",
    "  model_arch_pretrained: \"True\"\n",
    "  num_epochs: 7\n",
    "  learning_rate: 0.01\n",
    "  momentum: 0.01\n",
    "  register_model_as: \"dogs_dev\"\n",
    "  # register_model_as: \"places_dev\",\n",
    "  enable_profiling: \"False\"\n",
    "environment: azureml:AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu@latest\n",
    "compute: azureml:gpu-nc12sv3\n",
    "resources:\n",
    "  instance_count: 2\n",
    "  shm_size: 1000G\n",
    "display_name: pytorch_training_job\n",
    "experiment_name: pytorch_training_sample\n",
    "description: PyTorch training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86f966-a990-496b-93ee-43e20ebb1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create -f pytorch_training_job.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b9c5f",
   "metadata": {},
   "source": [
    "Once we create that job, we submit it through `MLClient`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf19ded",
   "metadata": {},
   "source": [
    "You can iterate on this design as much as you'd like, updating the local code of the job and re-submit the pipeline.\n",
    "\n",
    "Note: in the code above, we have commented out the lines you'd need to test this training job on the Places 2 dataset (1.8m images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782c68e",
   "metadata": {},
   "source": [
    "# 3. Analyze experiments using MLFlow\n",
    "\n",
    "Azure ML natively integrates with MLFlow so that if your code already supports MLFlow logging, you will not have to modify it to report your metrics within Azure ML. The component above is using MLFlow internally to report relevant metrics, logs and artifacts. Look for `mlflow` calls within the script `train.py`.\n",
    "\n",
    "To access this data in the Azure ML Studio, click on the component in the pipeline to open the Details panel, then choose the **Metrics** panel.\n",
    "\n",
    "You can also access those metrics programmatically using mlflow. We'll demo a couple examples below.\n",
    "\n",
    "## 3.1. Connect to Azure ML using MLFlow client\n",
    "\n",
    "Connecting to Azure ML using MLFlow required to `pip install azureml-mlflow mlflow` (both). You can use the `MLClient` to obtain a tracking uri to connect with the mlflow client. In the example below, we'll get all the runs related to the training experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af554a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlflow.set_tracking_uri(ml_client.workspaces.get().mlflow_tracking_uri)\n",
    "\n",
    "# search for the training step within the pipeline\n",
    "mlflow.set_experiment(\"e2e_image_sample\")\n",
    "\n",
    "# search for all runs and return as a pandas dataframe\n",
    "mlflow_runs = mlflow.search_runs()\n",
    "\n",
    "# display all runs as a dataframe in the notebook\n",
    "mlflow_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bea13",
   "metadata": {},
   "source": [
    "## 3.2. Analyze metrics accross multiple jobs\n",
    "\n",
    "You can also use mlflow to search all your runs, filter by some specific properties and get the results as a pandas dataframe. Once you get that dataframe, you can implement any analysis on top of it.\n",
    "\n",
    "Below, we're extracting all runs and show the effect of profiling on the epoch training time.\n",
    "\n",
    "![mlflow runs in a pandas dataframe](./media/pytorch_train_mlflow_runs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = mlflow.search_runs(\n",
    "    # we're using mlflow syntax to restrict to a specific parameter\n",
    "    filter_string=f\"params.model_arch = 'resnet18'\"\n",
    ")\n",
    "\n",
    "# we're keeping only some relevant columns\n",
    "columns = [\n",
    "    \"run_id\",\n",
    "    \"status\",\n",
    "    \"end_time\",\n",
    "    \"metrics.epoch_train_time\",\n",
    "    \"metrics.epoch_train_acc\",\n",
    "    \"metrics.epoch_valid_acc\",\n",
    "    \"params.enable_profiling\",\n",
    "]\n",
    "\n",
    "# showing the raw results in notebook\n",
    "runs[columns].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fc568",
   "metadata": {},
   "source": [
    "## 3.3. Analyze the metrics of a specific job\n",
    "\n",
    "Using MLFlow, you can retrieve all the metrics produces by a given run. You can then leverage any usual tool to draw the analysis that is relevant for you. In the example below, we're plotting accuracy per epoch.\n",
    "\n",
    "![plot training and validation accuracy over epochs](./media/pytorch_train_mlflow_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we're using the small scale training on validation data\n",
    "training_run_id = small_scale_run_id\n",
    "\n",
    "# alternatively, you can directly use a known training step id\n",
    "# training_run_id = \"...\"\n",
    "\n",
    "# open a client to get metric history\n",
    "client = MlflowClient()\n",
    "\n",
    "print(f\"Obtaining results for run id {training_run_id}\")\n",
    "\n",
    "# create a plot\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"epoch\")\n",
    "\n",
    "for metric in [\"epoch_train_acc\", \"epoch_valid_acc\"]:\n",
    "    # get all values taken by the metric\n",
    "    try:\n",
    "        metric_history = client.get_metric_history(training_run_id, metric)\n",
    "    except:\n",
    "        print(f\"Metric {metric} could not be found in history\")\n",
    "        continue\n",
    "\n",
    "    epochs = [metric_entry.step for metric_entry in metric_history]\n",
    "    metric_array = [metric_entry.value for metric_entry in metric_history]\n",
    "    ax.plot(epochs, metric_array, label=metric)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb98d44",
   "metadata": {},
   "source": [
    "## 3.4. Retrieve artifacts for local analysis (ex: tensorboard)\n",
    "\n",
    "MLFlow also allows you to record artifacts during training. The script `train.py` leverages the [PyTorch profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) to produce logs for analyzing GPU performance. It uses mlflow to record those logs as artifacts.\n",
    "\n",
    "To benefit from that, use the option `enable_profiling=True` in the submission code of section 2.\n",
    "\n",
    "In the following, we'll download those locally to inspect with other tools such as tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2796344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# here we're using the small scale training on validation data\n",
    "training_run_id = small_scale_run_id\n",
    "\n",
    "# alternatively, you can directly use a known training step id\n",
    "# training_run_id = \"...\"\n",
    "\n",
    "# open a client to get metric history\n",
    "client = MlflowClient()\n",
    "\n",
    "# create local directory to store artefacts\n",
    "os.makedirs(\"./logs/\", exist_ok=True)\n",
    "\n",
    "for artifact in client.list_artifacts(training_run_id, path=\"profiler/markdown/\"):\n",
    "    print(f\"Downloading artifact {artifact.path}\")\n",
    "    client.download_artifacts(training_run_id, path=artifact.path, dst_path=\"./logs\")\n",
    "else:\n",
    "    print(f\"No artefacts were found for profiler/markdown/ in run id {training_run_id}\")\n",
    "\n",
    "for artifact in client.list_artifacts(\n",
    "    training_run_id, path=\"profiler/tensorboard_logs/\"\n",
    "):\n",
    "    print(f\"Downloading artifact {artifact.path}\")\n",
    "    client.download_artifacts(training_run_id, path=artifact.path, dst_path=\"./logs\")\n",
    "else:\n",
    "    print(f\"No artefacts were found for profiler/markdown/ in run id {training_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a3912",
   "metadata": {},
   "source": [
    "We can now run tensorboard locally with the downloaded artifacts to run some analysis of GPU performance (see example snapshot below).\n",
    "\n",
    "```\n",
    "tensorboard --logdir=\"./logs/profiler/tensorboard_logs/\"\n",
    "```\n",
    "\n",
    "![tensorboard logs generated by pytorch profiler](./media/pytorch_train_tensorboard_logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f658d73",
   "metadata": {},
   "source": [
    "![](media/mlflow_plot.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
