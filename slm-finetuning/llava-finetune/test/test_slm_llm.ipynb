{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "endpoint = \"https://westusdeployment.openai.azure.com/\"\n",
    "deployment = \"gpt4o\"\n",
    "      \n",
    "      \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=\"cf79102ef923421cad64474d96c1a7ff\", \n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) \n",
    "\n",
    "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "api_key = 'GzindaZntfNJhoBTgkRE4DcpxMaf4fuR'\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'finetune-llava-v1-5-13b2-1' }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_slm(prompt, image_url):\n",
    "\n",
    "    data = {\"image_file\" : image_url,\n",
    "            \"image_prompt\" : prompt}\n",
    "\n",
    "\n",
    "\n",
    "    url = \"https://gpu-workspace-nvhsz.northeurope.inference.ml.azure.com/score\"\n",
    "\n",
    "    body = str.encode(json.dumps(data))\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "\n",
    "        result = response.read().decode(\"utf8\")\n",
    "        print(result)\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(error.read().decode(\"utf8\", 'ignore'))\n",
    "test_slm(\"What is this person doing?\", \"https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/74.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_llm(prompt, image_url):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}, \n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": image_url, \n",
    "                    \"detail\": \n",
    "                    \"low\"}}]} \n",
    "            ]\n",
    "    )\n",
    "\n",
    "    json_out = json.loads(completion.to_json())\n",
    "\n",
    "    msg = json_out[\"choices\"][-1][\"message\"][\"content\"]\n",
    "    print(msg)\n",
    "\n",
    "#test_llm(\"What is this person doing?\", \"https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/74.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/1.jpg\n",
      "[\"<s> playing video games, gaming, playing, wii bowling</s>\"]\n",
      "The person in the image is lying on an air mattress, facing downwards.\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/10.jpg\n",
      "[\"<s> playing, gaming, wii bowling, playing wii</s>\"]\n",
      "The person in the image is lying on the floor, appearing to have fallen or be in a position of distress.\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/100.jpg\n",
      "[\"<s> sleeping, laying, resting</s>\"]\n",
      "Skipping https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/100.jpg\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/13.jpg\n",
      "[\"<s> playing, playing with remote, playing wii</s>\"]\n",
      "The person in the image appears to be lying on the floor, possibly having fallen. It looks like they might be in distress or unconscious. There is also a bottle nearby, which could suggest a potential reason for their condition, but this is not clear. This situation seems to warrant immediate attention and assistance.\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/14.jpg\n",
      "[\"<s> sleeping, laying, resting</s>\"]\n",
      "The person in the photo appears to be lying on the floor next to an overturned office chair, with some scattered papers around. It looks like they might have fallen or collapsed.\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/15.jpg\n",
      "[\"<s> drinking, chugging, drink</s>\"]\n",
      "The person in the image appears to have fallen on the floor near a staircase. There is an overturned glass and a pair of glasses nearby, suggesting the person may have dropped these items during the fall. The person looks distressed or in pain, which could indicate they are experiencing discomfort from the fall.\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/16.jpg\n",
      "[\"<s> falling, tripping, stumbling</s>\"]\n",
      "The person in the image appears to have fallen down some stairs. They are lying face down on the floor with their legs still on the stairs, surrounded by scattered papers and folders. This suggests that they may have been carrying the items when they fell.\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/160323-seniorfallen-stock.jpg\n",
      "[\"<s> falling, laying, falling down</s>\"]\n",
      "The person in the image appears to be lying on the floor. It's possible that they have fallen and may need assistance. If this is an unexpected or emergency situation, please seek help immediately.\n",
      "https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/17.jpg\n",
      "[\"<s> stretching, bending, yawning</s>\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "fall_data_dir = \"C:/Users/antonslutsky/Dev/Data/fall-detection/fall_dataset/fall\"\n",
    "not_fall_data_dir = \"C:/Users/antonslutsky/Dev/Data/fall-detection/fall_dataset/not-fall\"\n",
    "\n",
    "fall_files = os.listdir(fall_data_dir)\n",
    "\n",
    "prompt= \"What is this person doing?\"\n",
    "\n",
    "with open(\"fall_out.csv\", \"w\") as out:\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            img = fall_files[i]\n",
    "            \n",
    "            img = f\"https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/{img}\"\n",
    "\n",
    "            print(img)\n",
    "\n",
    "            start = time.time()\n",
    "            test_slm(prompt, img)\n",
    "            end = time.time()\n",
    "            slm_time = end - start\n",
    "\n",
    "            start = time.time()\n",
    "            test_llm(prompt, img)\n",
    "            end = time.time()\n",
    "            llm_time = end - start\n",
    "\n",
    "            out.write(f\"{img},{slm_time},{llm_time}\\n\")\n",
    "        except:\n",
    "            print(f\"Skipping {img}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image_data):\n",
    "    encoded_string = base64.b64encode(image_data).decode(\"utf8\")\n",
    "    image_file = 'data:image/jpeg;base64,' + encoded_string\n",
    "    return image_file\n",
    "\n",
    "with open(\"C:/Users/antonslutsky/Dev/Data/fall-detection/fall_dataset/fall/22.jpg\", \"rb\") as image_file:\n",
    "    image_file = encode_image(image_file.read())\n",
    "\n",
    "    print(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "img_base64_pref = 'data:image/jpeg;base64,'\n",
    "\n",
    "if image_file.startswith(img_base64_pref):\n",
    "    img_data = image_file[len(img_base64_pref):]\n",
    "    print(\"Image data:\", img_data)\n",
    "    msg = base64.b64decode(img_data)\n",
    "    buf = io.BytesIO(msg)\n",
    "    image = Image.open(buf).convert('RGB')\n",
    "\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from io import BytesIO\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "def cv2_to_pil(img): #Since you want to be able to use Pillow (PIL)\n",
    "    return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    cv2.imshow(\"test\", frame)\n",
    "\n",
    "    pil_image = cv2_to_pil(frame)\n",
    "\n",
    "    im_file = BytesIO()\n",
    "    pil_image.save(im_file, format=\"JPEG\")\n",
    "    im_bytes = im_file.getvalue()  # im_bytes: image in binary format.\n",
    "    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n",
    "\n",
    "    image_file = 'data:image/jpeg;base64,' + im_b64\n",
    "\n",
    "    test_slm(\"What do you see?\", image_file)\n",
    "\n",
    "    #break\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"opencv_frame_{}.jpg\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "many_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
