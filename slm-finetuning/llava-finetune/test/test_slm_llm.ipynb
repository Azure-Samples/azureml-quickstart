{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=\"a5455f062a5c422ab8fa8e062713601c\", region=\"westus\")\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The neural multilingual voice can speak different languages based on the input text.\n",
    "speech_config.speech_synthesis_voice_name='en-US-AvaMultilingualNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "\n",
    "def speak(text = \"hello\"):\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "endpoint = \"https://westusdeployment.openai.azure.com/\"\n",
    "deployment = \"gpt4o\"\n",
    "      \n",
    "      \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=\"cf79102ef923421cad64474d96c1a7ff\", \n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) \n",
    "\n",
    "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "api_key = 'Tcxc8Mf8v1pV1gB1CA1wC9H4h2HxS0wj'\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'finetune-llava-v1-5-13b2-1' }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"0\": \"<s> falling, tripping, falling down stairs</s>\"}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"0\": \"<s> falling, tripping, falling down stairs</s>\"}]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_slm(prompt, image_url):\n",
    "\n",
    "    data = {\"image_file\" : image_url,\n",
    "            \"image_prompt\" : prompt}\n",
    "\n",
    "\n",
    "\n",
    "    url = \"https://gpu-workspace-bblaa.northeurope.inference.ml.azure.com/score\"\n",
    "\n",
    "    body = str.encode(json.dumps(data))\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "\n",
    "        result = response.read().decode(\"utf8\")\n",
    "        print(result)\n",
    "        return result\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(error.read().decode(\"utf8\", 'ignore'))\n",
    "test_slm(\"What is this person doing?\", \"https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/74.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_llm(prompt, image_url):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}, \n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": image_url, \n",
    "                    \"detail\": \n",
    "                    \"low\"}}]} \n",
    "            ]\n",
    "    )\n",
    "\n",
    "    json_out = json.loads(completion.to_json())\n",
    "\n",
    "    msg = json_out[\"choices\"][-1][\"message\"][\"content\"]\n",
    "    print(msg)\n",
    "\n",
    "#test_llm(\"What is this person doing?\", \"https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/74.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import time\n",
    "\n",
    "\n",
    "\n",
    "# fall_data_dir = \"C:/Users/antonslutsky/Dev/Data/fall-detection/fall_dataset/fall\"\n",
    "# not_fall_data_dir = \"C:/Users/antonslutsky/Dev/Data/fall-detection/fall_dataset/not-fall\"\n",
    "\n",
    "# fall_files = os.listdir(fall_data_dir)\n",
    "\n",
    "# prompt= \"What is this person doing?\"\n",
    "\n",
    "# #with open(\"fall_out.csv\", \"w\") as out:\n",
    "# if True:\n",
    "#     for i in range(20):\n",
    "#         try:\n",
    "#             img = fall_files[i]\n",
    "            \n",
    "#             img = f\"https://raw.githubusercontent.com/ambianic/fall-detection/main/fall_dataset/fall/{img}\"\n",
    "\n",
    "#             print(img)\n",
    "\n",
    "#             start = time.time()\n",
    "#             test_slm(prompt, img)\n",
    "#             end = time.time()\n",
    "#             slm_time = end - start\n",
    "\n",
    "#             start = time.time()\n",
    "#             test_llm(prompt, img)\n",
    "#             end = time.time()\n",
    "#             llm_time = end - start\n",
    "\n",
    "#             #out.write(f\"{img},{slm_time},{llm_time}\\n\")\n",
    "#         except:\n",
    "#             print(f\"Skipping {img}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image_data):\n",
    "    encoded_string = base64.b64encode(image_data).decode(\"utf8\")\n",
    "    image_file = 'data:image/jpeg;base64,' + encoded_string\n",
    "    return image_file\n",
    "\n",
    "with open(\"C:/Users/antonslutsky/Dev/Data/fall-detection/fall_dataset/fall/22.jpg\", \"rb\") as image_file:\n",
    "    image_file = encode_image(image_file.read())\n",
    "\n",
    "    print(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "img_base64_pref = 'data:image/jpeg;base64,'\n",
    "\n",
    "if image_file.startswith(img_base64_pref):\n",
    "    img_data = image_file[len(img_base64_pref):]\n",
    "    print(\"Image data:\", img_data)\n",
    "    msg = base64.b64decode(img_data)\n",
    "    buf = io.BytesIO(msg)\n",
    "    image = Image.open(buf).convert('RGB')\n",
    "\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"0\": \"<s> man, beard, staring</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, light, fan</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, staring</s>\"}]\n",
      "[{\"0\": \"<s> man, beard</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, light, fan</s>\"}]\n",
      "[{\"0\": \"<s> fan, ceiling fan</s>\"}]\n",
      "[{\"0\": \"<s> man, thumbs up</s>\"}]\n",
      "Speech synthesized for text [Thumbs up!]\n",
      "[{\"0\": \"<s> man, thumbs up</s>\"}]\n",
      "Speech synthesized for text [Thumbs up!]\n",
      "[{\"0\": \"<s> man, beard, mustache</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, mustache</s>\"}]\n",
      "[{\"0\": \"<s> man, beard</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, blue shirt</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, blue shirt</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, smiling</s>\"}]\n",
      "Speech synthesized for text [Looks like you are happy.]\n",
      "[{\"0\": \"<s> man, beard</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, light, fan</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, lightbulb, fan</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, blue shirt</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, blue shirt</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, lightbulb, fan</s>\"}]\n",
      "[{\"0\": \"<s> man, beard, mustache</s>\"}]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from io import BytesIO\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "def cv2_to_pil(img): #Since you want to be able to use Pillow (PIL)\n",
    "    return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "\n",
    "    pil_image = cv2_to_pil(frame)\n",
    "\n",
    "    im_file = BytesIO()\n",
    "    pil_image.save(im_file, format=\"JPEG\")\n",
    "    im_bytes = im_file.getvalue()  # im_bytes: image in binary format.\n",
    "    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n",
    "\n",
    "    image_file = 'data:image/jpeg;base64,' + im_b64\n",
    "\n",
    "    res = test_slm(\"What do you see?\", image_file)\n",
    "\n",
    "    if \"thumbs up\" in res:\n",
    "        speak(\"Thumbs up!\")\n",
    "        #break\n",
    "    if \"smiling\" in res:\n",
    "        speak(\"Looks like you are happy.\")\n",
    "\n",
    "    cv2.imshow(\"test\", frame)\n",
    "\n",
    "    #break\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"opencv_frame_{}.jpg\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "many_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
