{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile finetune_phi3_job.yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n",
    "code: ./src\n",
    "\n",
    "command: >- \n",
    "  python finetune_phi3.py ${{inputs.data_dir}} ${{outputs.out_dir}}\n",
    "inputs:\n",
    "  data_dir: \n",
    "    type: uri_folder\n",
    "    #mode: ro_mount\n",
    "    #path: azureml:burbery_data@latest\n",
    "    path: azureml:Sujet-Finance-Vision-10k@latest\n",
    "outputs:\n",
    "  out_dir: \n",
    "    type: custom_model\n",
    "    mode: upload\n",
    "environment: azureml:llava_finetuning:16\n",
    "environment_variables:\n",
    "    WANDB_MODE: disabled\n",
    "resources:\n",
    "  instance_count: 1\n",
    "distribution:\n",
    "  type: pytorch \n",
    "  process_count_per_instance: 1\n",
    "services:\n",
    "    my_vs_code:\n",
    "      type: vs_code\n",
    "      nodes: all # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n",
    "    my_jupyter_lab:\n",
    "      type: jupyter_lab\n",
    "      nodes: all\n",
    "#compute: azureml:fine-tune-cluster\n",
    "compute: azureml:a100-low-priority-france\n",
    "display_name: finetune_phi3_vision_job_5\n",
    "#name: finetune_phi3_vision_job\n",
    "experiment_name: finetune_phi3_vision\n",
    "description: Finetune Phi3 Vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create -f finetune_phi3_job.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile endpoint_phi3_env_jo.yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n",
    "command: >- \n",
    "  echo \"Model Dir: ${{inputs.model_dir}}\"; sleep 6000\n",
    "inputs:\n",
    "  model_dir: \n",
    "    type: custom_model\n",
    "    path: azureml:finetuned_phi3_vision@latest\n",
    "environment: azureml:llava_finetuning_inference:2\n",
    "environment_variables:\n",
    "    WANDB_MODE: disabled\n",
    "resources:\n",
    "  instance_count: 1\n",
    "distribution:\n",
    "  type: pytorch \n",
    "  process_count_per_instance: 1\n",
    "services:\n",
    "    my_vs_code:\n",
    "      type: vs_code\n",
    "      nodes: all # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n",
    "    my_jupyter_lab:\n",
    "      type: jupyter_lab\n",
    "      nodes: all\n",
    "compute: azureml:fine-tune-cluster\n",
    "#compute: azureml:a100-low-priority\n",
    "display_name: endpoint_phi3_env_job\n",
    "#name: endpoint_phi3_env_job\n",
    "experiment_name: finetune_phi3_vision\n",
    "description: Endpoint Phi3 Vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create -f endpoint_phi3_env_jo.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./endpoint/endpoint.yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\n",
    "name: finetunedPhi3EndpointFin\n",
    "auth_mode: key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml online-endpoint create --file ./endpoint/endpoint.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./endpoint/deployment.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./endpoint/deployment.yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: blue\n",
    "endpoint_name: finetunedPhi3EndpointFin\n",
    "model: azureml:phi3_finetuned_financial:1\n",
    "code_configuration:\n",
    "  code: .\n",
    "  scoring_script: score.py\n",
    "environment: azureml:llava_finetuning_inference:2\n",
    "instance_type: Standard_NC12s_v3\n",
    "instance_count: 1\n",
    "request_settings:\n",
    "  request_timeout_ms: 180000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................{\n",
      "  \"app_insights_enabled\": false,\n",
      "  \"code_configuration\": {\n",
      "    \"code\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/codes/8544249c-ba30-41c8-a033-5620519fc961/versions/1\",\n",
      "    \"scoring_script\": \"score.py\"\n",
      "  },\n",
      "  \"egress_public_network_access\": \"enabled\",\n",
      "  \"endpoint_name\": \"finetunedphi3endpointfin\",\n",
      "  \"environment\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/environments/llava_finetuning_inference/versions/2\",\n",
      "  \"environment_variables\": {\n",
      "    \"AML_APP_ROOT\": \"/var/azureml-app/endpoint\",\n",
      "    \"AZUREML_ENTRY_SCRIPT\": \"score.py\",\n",
      "    \"AZUREML_MODEL_DIR\": \"/var/azureml-app/azureml-models/phi3_finetuned_financial/1\"\n",
      "  },\n",
      "  \"id\": \"/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/onlineEndpoints/finetunedphi3endpointfin/deployments/blue\",\n",
      "  \"instance_count\": 1,\n",
      "  \"instance_type\": \"Standard_NC12s_v3\",\n",
      "  \"liveness_probe\": {\n",
      "    \"failure_threshold\": 30,\n",
      "    \"initial_delay\": 10,\n",
      "    \"period\": 10,\n",
      "    \"success_threshold\": 1,\n",
      "    \"timeout\": 2\n",
      "  },\n",
      "  \"model\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/models/phi3_finetuned_financial/versions/1\",\n",
      "  \"name\": \"blue\",\n",
      "  \"properties\": {},\n",
      "  \"provisioning_state\": \"Succeeded\",\n",
      "  \"readiness_probe\": {\n",
      "    \"failure_threshold\": 30,\n",
      "    \"initial_delay\": 10,\n",
      "    \"period\": 10,\n",
      "    \"success_threshold\": 1,\n",
      "    \"timeout\": 2\n",
      "  },\n",
      "  \"request_settings\": {\n",
      "    \"max_concurrent_requests_per_instance\": 1,\n",
      "    \"request_timeout_ms\": 180000\n",
      "  },\n",
      "  \"resourceGroup\": \"antonslutsky-rg\",\n",
      "  \"scale_settings\": {\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"tags\": {},\n",
      "  \"type\": \"managed\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All traffic will be set to deployment blue once it has been provisioned.\n",
      "If you interrupt this command or it times out while waiting for the provisioning, you can try to set all the traffic to this deployment later once its has been provisioned.\n",
      "Check: endpoint finetunedPhi3EndpointFin exists\n",
      "\n",
      "Uploading endpoint (0.0 MBs):   0%|          | 0/4935 [00:00<?, ?it/s]\n",
      "Uploading endpoint (0.0 MBs):   8%|8         | 399/4935 [00:00<00:01, 2298.36it/s]\n",
      "Uploading endpoint (0.0 MBs): 100%|##########| 4935/4935 [00:00<00:00, 6779.65it/s]\n",
      "Uploading endpoint (0.0 MBs): 100%|##########| 4935/4935 [00:00<00:00, 6473.50it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment create --all-traffic --file ./endpoint/deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"output\":\"None\"}'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "# data = {\n",
    "#     \"prompt\" : \"<|user|>\\n<|image_1|>What is shown in this image?<|end|><|assistant|>\\n\",\n",
    "#     \"image_url\" : \"https://templatelab.com/wp-content/uploads/2016/06/Personal-Financial-Statement-Template-31.jpg\"\n",
    "# }\n",
    "\n",
    "data = {\"input_data\": {\"input_string\": [\"hello\"]}, \"parameters\": {\"top_p\": 1.0, \"temperature\": 1.0, \"max_new_tokens\": 500}}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://aml-westus2-phi3v-pihof.westus2.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "api_key = 'cERxKDSbQHQhkY4cqziMZMiCgM9bSz9u'\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "many_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
