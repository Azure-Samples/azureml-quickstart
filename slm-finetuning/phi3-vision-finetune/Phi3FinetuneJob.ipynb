{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antonslutsky\\AppData\\Local\\anaconda3\\envs\\many_models\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34776\\3147489236.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# Code orginally from https://wandb.ai/byyoung3/mlnews3/reports/How-to-fine-tune-Phi-3-vision-on-a-custom-dataset--Vmlldzo4MTEzMTg3 \n",
    "# Credits to: Brett Young https://github.com/bdytx5/\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import resize, to_pil_image\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "run = wandb.init(project=\"burberry-product-phi3\", entity=\"byyoung3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for Burberry Product Prices and Images\n",
    "class BurberryProductDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length, image_size):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.padding_side = 'left'  # Set padding side to left\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row at the given index\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Create the text input for the model\n",
    "        text = f\"<|user|>\\n<|image_1|>What is shown in this image?<|end|><|assistant|>\\nProduct: {row['title']}, Category: {row['category3_code']}, Full Price: {row['full_price']}<|end|>\"\n",
    "        \n",
    "        # Get the image path from the row\n",
    "        image_path = row['local_image_path']\n",
    "        \n",
    "        # Tokenize the text input\n",
    "        encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        \n",
    "        try:\n",
    "            # Load and transform the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.image_transform_function(image)\n",
    "        except (FileNotFoundError, IOError):\n",
    "            # Skip the sample if the image is not found\n",
    "            return None\n",
    "        \n",
    "        # Add the image and price information to the encodings dictionary\n",
    "        encodings['pixel_values'] = image\n",
    "        encodings['price'] = row['full_price']\n",
    "        \n",
    "        return {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "\n",
    "    def image_transform_function(self, image):\n",
    "        # Convert the image to a numpy array\n",
    "        image = np.array(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from disk\n",
    "dataset_path = './data/burberry_dataset/burberry_dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Initialize processor and tokenizer for the pre-trained model\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.9 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "train_indices, val_indices = random_split(range(len(df)), [train_size, val_size])\n",
    "train_indices = train_indices.indices\n",
    "val_indices = val_indices.indices\n",
    "train_df = df.iloc[train_indices]\n",
    "val_df = df.iloc[val_indices]\n",
    "\n",
    "# Create dataset and dataloader for training set\n",
    "train_dataset = BurberryProductDataset(train_df, tokenizer, max_length=512, image_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Create dataset and dataloader for validation set\n",
    "val_dataset = BurberryProductDataset(val_df, tokenizer, max_length=512, image_size=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "eval_interval = 150  # Evaluate every 'eval_interval' steps\n",
    "loss_scaling_factor = 1000.0  # Variable to scale the loss by a certain amount\n",
    "save_dir = './saved_models'\n",
    "step = 0\n",
    "accumulation_steps = 64  # Accumulate gradients over this many steps\n",
    "\n",
    "# Create a directory to save the best model\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = None\n",
    "\n",
    "# Select 10 random images from the validation set for logging\n",
    "num_log_samples = 10\n",
    "log_indices = random.sample(range(len(val_dataset)), num_log_samples)\n",
    "\n",
    "# Function to extract the predicted price from model predictions\n",
    "def extract_price_from_predictions(predictions, tokenizer):\n",
    "    # Assuming the price is at the end of the text and separated by a space\n",
    "    predicted_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        predicted_price = float(predicted_text.split()[-1].replace(',', ''))\n",
    "    except ValueError:\n",
    "        predicted_price = 0.0\n",
    "    return predicted_price\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def evaluate(model, val_loader, device, tokenizer, step, log_indices, max_samples=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_price_error = 0\n",
    "    log_images = []\n",
    "    log_gt_texts = []\n",
    "    log_pred_texts = []\n",
    "    table = wandb.Table(columns=[\"Image\", \"Ground Truth Text\", \"Predicted Text\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "\n",
    "            if batch is None:  # Skip if the batch is None\n",
    "                continue\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = input_ids.clone().detach()\n",
    "            actual_price = batch['price'].item()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                pixel_values=pixel_values, \n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate price error\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            predicted_price = extract_price_from_predictions(predictions, tokenizer)\n",
    "            price_error = abs(predicted_price - actual_price)\n",
    "            total_price_error += price_error\n",
    "\n",
    "            # Log images, ground truth texts, and predicted texts\n",
    "            if i in log_indices:\n",
    "                log_images.append(pixel_values.cpu().squeeze().numpy())\n",
    "                log_gt_texts.append(tokenizer.decode(labels[0], skip_special_tokens=True))\n",
    "                log_pred_texts.append(tokenizer.decode(predictions[0], skip_special_tokens=True))\n",
    "\n",
    "                # Convert image to PIL format\n",
    "                pil_img = to_pil_image(resize(torch.from_numpy(log_images[-1]).permute(2, 0, 1), (336, 336))).convert(\"RGB\")\n",
    "                \n",
    "                # Add data to the table\n",
    "                table.add_data(wandb.Image(pil_img), log_gt_texts[-1], log_pred_texts[-1])\n",
    "\n",
    "                # Log the table incrementally\n",
    "    wandb.log({\"Evaluation Results step {}\".format(step): table, \"Step\": step})\n",
    "\n",
    "    avg_loss = total_loss / (i + 1)  # i+1 to account for the loop index\n",
    "    avg_price_error = total_price_error / (i + 1)\n",
    "    model.train()\n",
    "\n",
    "    return avg_loss, avg_price_error\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop for the specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    total_train_price_error = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        step += 1\n",
    "\n",
    "        if batch is None:  # Skip if the batch is None\n",
    "            continue\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = input_ids.clone().detach()\n",
    "        actual_price = batch['price'].float().to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            pixel_values=pixel_values, \n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss = loss\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)            \n",
    "        predicted_price = extract_price_from_predictions(predictions, tokenizer)\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        if (step % accumulation_steps) == 0:\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad /= accumulation_steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_train_price_error += abs(predicted_price - actual_price.item())\n",
    "        batch_count += 1\n",
    "\n",
    "        # Log batch loss to Weights & Biases\n",
    "        wandb.log({\"Batch Loss\": total_loss.item(), \"Step\": step})\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Step: {step}, Batch Loss: {total_loss.item()}\")\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            val_loss, val_price_error = evaluate(model, val_loader, device, tokenizer=tokenizer, log_indices=log_indices, step=step )\n",
    "            wandb.log({\n",
    "                \"Validation Loss\": val_loss,\n",
    "                \"Validation Price Error (Average)\": val_price_error,\n",
    "                \"Step\": step\n",
    "            })\n",
    "            print(f\"Step: {step}, Validation Loss: {val_loss}, Validation Price Error (Normalized): {val_price_error}\")\n",
    "\n",
    "            # Save the best model based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_path = os.path.join(save_dir, f\"best_model\")\n",
    "                model.save_pretrained(best_model_path, safe_serialization=False)\n",
    "                tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "            avg_train_loss = total_train_loss / batch_count\n",
    "            avg_train_price_error = total_train_price_error / batch_count\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Average Training Loss\": avg_train_loss,\n",
    "                \"Average Training Price Error\": avg_train_price_error\n",
    "            })\n",
    "            \n",
    "    print(f\"Epoch: {epoch}, Average Training Loss: {avg_train_loss}, Average Training Price Error: {avg_train_price_error}\")\n",
    "\n",
    "    # Log the best model to Weights & Biases\n",
    "    if best_model_path:\n",
    "        run.log_model(\n",
    "            path=best_model_path,\n",
    "            name=\"phi3-v-burberry\",\n",
    "            aliases=[\"best\"],\n",
    "        )\n",
    "\n",
    "# Finish the Weights & Biases run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "many_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
