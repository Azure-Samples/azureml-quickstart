{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from src.convert_image_to_ascii_art import convert_image_to_ascii\n",
    "import cv2\n",
    "\n",
    "capture = cv2.VideoCapture('C:\\\\Users\\\\antonslutsky\\\\Downloads\\\\2024_03_29_10_17_20_audio_video.mp4')\n",
    "\n",
    "frameNr = 0\n",
    "\n",
    "\n",
    "while (True):\n",
    "    success, frame = capture.read()\n",
    "    print(frameNr)\n",
    "    if success and frameNr == 10:\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        im_pil = Image.fromarray(img)\n",
    "        ascii_art = convert_image_to_ascii(im_pil)\n",
    "        print(ascii_art)\n",
    "        break\n",
    "    # else:\n",
    "    #     break\n",
    "    \n",
    "\n",
    "    frameNr = frameNr+1\n",
    " \n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "\n",
    "# Initialize video parameters\n",
    "SCREEN_SIZE = tuple(pyautogui.size())\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "fps = 12.0\n",
    "record_seconds = 10  # Set the desired recording duration\n",
    "\n",
    "# Create the video writer object\n",
    "out = cv2.VideoWriter(f\"output.avi\", fourcc, fps, SCREEN_SIZE)\n",
    "\n",
    "# Main loop for capturing screenshots and writing frames\n",
    "for i in range(int(record_seconds * fps)):\n",
    "    img = pyautogui.screenshot()\n",
    "    frame = np.array(img)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"screenshot\", frame)\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mouse\n",
    "import keyboard\n",
    "from datetime import datetime\n",
    "\n",
    "mouse_events = []\n",
    "keyboard_events = []\n",
    "\n",
    "def keyboard_event_hook(event):\n",
    "    keyboard_events.append((event.time, event.to_json()))\n",
    "\n",
    "\n",
    "mouse.hook(mouse_events.append)\n",
    "keyboard.hook(keyboard_event_hook)\n",
    "keyboard.start_recording()  \n",
    "\n",
    "keyboard.wait(\"a\")\n",
    "\n",
    "mouse.unhook(mouse_events.append)\n",
    "keyboard.unhook(keyboard_event_hook)\n",
    "keyboard.stop_recording()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(keyboard.KeyboardEvent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def video_to_frames(input_loc, output_loc):\n",
    "    vidcap = cv2.VideoCapture(input_loc)\n",
    "    success, image = vidcap.read()\n",
    "    print(success)\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        cv2.imwrite(f\"{output_loc}/frame{count}.jpg\", image)  # Save frame as JPEG file\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "# Example usage:\n",
    "video_to_frames(\"C:/Users/antonslutsky/Dev/azureml-quickstart/spark_pipeline/src/output2.mp4\", 'output_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login --tenant 16b3c013-d300-468d-ac64-7eda0820b6d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml component create --file ./jobs/extract_frames_job.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create --file ./jobs/extract_frames_job.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create --file ./jobs/convert_to_ascii_art_spark_job.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml component create --file ./jobs/convert_to_ascii_art_spark_job.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"$schema\": \"http://azureml/sdk-2-0/SparkJob.json\",\n",
      "  \"args\": \"--frames_ascii_dir ${{inputs.frames_ascii_dir}} --captured_data ${{inputs.captured_data}} --captured_data_training ${{outputs.captured_data_training}}\",\n",
      "  \"code\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/codes/f7d9f246-9ea8-473d-b038-f426a808b510/versions/1\",\n",
      "  \"conf\": {\n",
      "    \"spark.driver.cores\": 8,\n",
      "    \"spark.driver.memory\": \"56g\",\n",
      "    \"spark.executor.cores\": 8,\n",
      "    \"spark.executor.instances\": 12,\n",
      "    \"spark.executor.memory\": \"56g\"\n",
      "  },\n",
      "  \"creation_context\": {\n",
      "    \"created_at\": \"2024-04-02T18:25:05.160635+00:00\",\n",
      "    \"created_by\": \"Anton Slutsky\",\n",
      "    \"created_by_type\": \"User\",\n",
      "    \"last_modified_at\": \"2024-04-02T18:25:05.241871+00:00\",\n",
      "    \"last_modified_by\": \"Anton Slutsky\",\n",
      "    \"last_modified_by_type\": \"User\"\n",
      "  },\n",
      "  \"entry\": {\n",
      "    \"file\": \"generate_finetune_training_data_spark.py\"\n",
      "  },\n",
      "  \"environment\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/environments/CliV2AnonymousEnvironment/versions/fc46c80de9d6d9616bc2aa249a421c50\",\n",
      "  \"id\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/components/generate_finetune_training_data_spark1/versions/2024-04-02-18-25-03-4932111\",\n",
      "  \"inputs\": {\n",
      "    \"captured_data\": {\n",
      "      \"optional\": false,\n",
      "      \"type\": \"uri_folder\"\n",
      "    },\n",
      "    \"frames_ascii_dir\": {\n",
      "      \"optional\": false,\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"is_deterministic\": true,\n",
      "  \"name\": \"generate_finetune_training_data_spark1\",\n",
      "  \"outputs\": {\n",
      "    \"captured_data_training\": {\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"resourceGroup\": \"antonslutsky-rg\",\n",
      "  \"type\": \"spark\",\n",
      "  \"version\": \"2024-04-02-18-25-03-4932111\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'azureml:opencv_environment@latest' will not be used for anonymous registration\n",
      "\n",
      "Uploading src (0.04 MBs):   0%|          | 0/35494 [00:00<?, ?it/s]\n",
      "Uploading src (0.04 MBs):   5%|4         | 1702/35494 [00:00<00:06, 5007.71it/s]\n",
      "Uploading src (0.04 MBs): 100%|##########| 35494/35494 [00:00<00:00, 80749.77it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!az ml component create --file ./jobs/generate_finetune_training_data_job.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml job create --file ./jobs/finetune_llm_job.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"$schema\": \"https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\",\n",
      "  \"code\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/codes/d20f40f0-0ac1-40db-94df-6f2295f1bf44/versions/1\",\n",
      "  \"command\": \"python preprocess.py --task_name TextGeneration --text_key '${{inputs.text_key}}' $[[--ground_truth_key '${{inputs.ground_truth_key}}']] $[[--batch_size '${{inputs.batch_size}}']] $[[--pad_to_max_length '${{inputs.pad_to_max_length}}']] $[[--max_seq_length '${{inputs.max_seq_length}}']] $[[--train_file_path '${{inputs.train_file_path}}']] $[[--validation_file_path '${{inputs.validation_file_path}}']] $[[--test_file_path '${{inputs.test_file_path}}']] $[[--train_mltable_path '${{inputs.train_mltable_path}}']] $[[--validation_mltable_path '${{inputs.validation_mltable_path}}']] $[[--test_mltable_path '${{inputs.test_mltable_path}}']] --model_selector_output '${{inputs.model_selector_output}}' $[[--system_properties '${{inputs.system_properties}}']] --output_dir '${{outputs.output_dir}}'\",\n",
      "  \"creation_context\": {\n",
      "    \"created_at\": \"2024-04-02T18:21:36.823242+00:00\",\n",
      "    \"created_by\": \"Anton Slutsky\",\n",
      "    \"created_by_type\": \"User\",\n",
      "    \"last_modified_at\": \"2024-04-02T18:21:36.894844+00:00\",\n",
      "    \"last_modified_by\": \"Anton Slutsky\",\n",
      "    \"last_modified_by_type\": \"User\"\n",
      "  },\n",
      "  \"description\": \"Component to preprocess data for text generation task\",\n",
      "  \"display_name\": \"New Text Generation DataPreProcess\",\n",
      "  \"environment\": \"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/46\",\n",
      "  \"id\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/antonslutsky-rg/providers/Microsoft.MachineLearningServices/workspaces/gpu-workspace/components/new_text_generation_datapreprocess/versions/2024-04-02-18-21-34-9053649\",\n",
      "  \"inputs\": {\n",
      "    \"batch_size\": {\n",
      "      \"default\": \"1000\",\n",
      "      \"description\": \"Number of examples to batch before calling the tokenization function\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"ground_truth_key\": {\n",
      "      \"description\": \"key for ground_truth in an example. we take separate column for ground_truth to enable use cases like summarization, translation, question_answering, etc. which can be repurposed in form of text-generation where both text and ground_truth are needed. This separation is useful for calculating metrics. for eg. \\\"text\\\"=\\\"Summarize this dialog:\\\\n{input_dialogue}\\\\nSummary:\\\\n\\\", \\\"ground_truth\\\"=\\\"{summary of the dialogue}\\\"\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"max_seq_length\": {\n",
      "      \"default\": \"-1\",\n",
      "      \"description\": \"Default is -1 which means the padding is done up to the model's max length. Else will be padded to `max_seq_length`.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"model_selector_output\": {\n",
      "      \"description\": \"output folder of model selector containing model metadata like config, checkpoints, tokenizer config\",\n",
      "      \"optional\": false,\n",
      "      \"type\": \"uri_folder\"\n",
      "    },\n",
      "    \"pad_to_max_length\": {\n",
      "      \"default\": \"False\",\n",
      "      \"description\": \"If set to True, the returned sequences will be padded according to the model's padding side and padding index, up to their `max_seq_length`. If no `max_seq_length` is specified, the padding is done up to the model's max length.\",\n",
      "      \"enum\": [\n",
      "        \"true\",\n",
      "        \"false\"\n",
      "      ],\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"system_properties\": {\n",
      "      \"description\": \"Validation parameters propagated from pipeline.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"test_file_path\": {\n",
      "      \"description\": \"Path to the registered test data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"test_mltable_path\": {\n",
      "      \"description\": \"Path to the registered test data asset in `mltable` format.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"mltable\"\n",
      "    },\n",
      "    \"text_key\": {\n",
      "      \"description\": \"key for text in an example. format your data keeping in mind that text is concatenated with ground_truth while finetuning in the form - text + groundtruth. for eg. \\\"text\\\"=\\\"knock knock\\\\n\\\", \\\"ground_truth\\\"=\\\"who's there\\\"; will be treated as \\\"knock knock\\\\nwho's there\\\"\",\n",
      "      \"optional\": false,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"train_file_path\": {\n",
      "      \"description\": \"Path to the registered training data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"train_mltable_path\": {\n",
      "      \"description\": \"Path to the registered training data asset in `mltable` format.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"mltable\"\n",
      "    },\n",
      "    \"validation_file_path\": {\n",
      "      \"description\": \"Path to the registered validation data asset. The supported data formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"validation_mltable_path\": {\n",
      "      \"description\": \"Path to the registered validation data asset in `mltable` format.\",\n",
      "      \"optional\": true,\n",
      "      \"type\": \"mltable\"\n",
      "    }\n",
      "  },\n",
      "  \"is_deterministic\": true,\n",
      "  \"name\": \"new_text_generation_datapreprocess\",\n",
      "  \"outputs\": {\n",
      "    \"output_dir\": {\n",
      "      \"description\": \"The folder contains the tokenized output of the train, validation and test data along with the tokenizer files used to tokenize the data\",\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"resourceGroup\": \"antonslutsky-rg\",\n",
      "  \"resources\": {\n",
      "    \"instance_count\": 1\n",
      "  },\n",
      "  \"type\": \"command\",\n",
      "  \"version\": \"2024-04-02-18-21-34-9053649\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az ml component create --file ./jobs/data_preprocess_job.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
